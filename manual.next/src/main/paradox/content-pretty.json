[
  {
    "name": "about.md",
    "id": "/about.md",
    "url": "/about.html",
    "title": "About Otoroshi",
    "content": "# About Otoroshi\n\nAt the beginning of 2017, we had the need to create a new environment to be able to create new \"digital\" products very quickly in an agile fashion at <a href=\"https://www.maif.fr/\" target=\"_blank\">MAIF</a>. Naturally we turned to PaaS solutions and chose the excellent <a href=\"https://www.clever-cloud.com/\">Clever-Cloud</a> product to run our apps. \n\nWe also chose that every feature team will have the freedom to choose its own technological stack to build its product. It was a nice move but it has also introduced some challenges in terms of homogeneity for traceability, security, logging, ... because we did not want to force library usage in the products. We could have used something like <a href=\"http://philcalcado.com/2017/08/03/pattern_service_mesh.html\" target=\"_blank\">Service Mesh Pattern</a> but the deployement model of <a href=\"https://www.clever-cloud.com/\">Clever-Cloud</a> prevented us to do it.\n\nThe right solution was to use a reverse proxy or some kind of API Gateway able to provide tracability, logging, security with apikeys, quotas, DNS as a service locator, etc. We needed something easy to use, with a human friendly UI, a nice API to extends its features, true hot reconfiguration, able to generate internal events for third party usage. A couple of solutions were available at that time, but not one seems to fit our needs, there was always something missing, too complicated for our needs or not playing well with <a href=\"https://www.clever-cloud.com/\">Clever-Cloud</a> deployment model.\n\nAt some point, we tried to write a small prototype to explore what could be our dream reverse proxy. The design was very simple, there were some rough edges but every major feature needed was there waiting to be enhanced.\n\n**Otoroshi** was born and we decided to move ahead with our hairy monster :)\n\n## Philosophy \n\nEvery OSS product build at <a href=\"https://www.maif.fr/\" target=\"_blank\">MAIF</a> like <a href=\"https://maif.github.io/izanami/\" target=\"_blank\">Izanami</a> follow a common philosophy. \n\n* the services or API provided should be technology agnostic.\n* http first: http is the right answer to the previous quote   \n* api First: The UI is just another client of the api. \n* secured: The services exposed need authentication for both humans or machines  \n* event based: The services should expose a way to get notified of what happened inside.  \n"
  },
  {
    "name": "api.md",
    "id": "/api.md",
    "url": "/api.html",
    "title": "Admin REST API",
    "content": "# Admin REST API\n\nOtoroshi provides a fully featured REST admin API to perform almost every operation possible in the Otoroshi dashboard. The Otoroshi dashbaord is just a regular consumer of the admin API.\n\nUsing the admin API, you can do whatever you want and enhance your Otoroshi instances with a lot of features that will feet your needs.\n\n## Swagger descriptor\n\nThe Otoroshi admin API is described using OpenAPI format and is available at :\n\nhttps://maif.github.io/otoroshi/manual/code/openapi.json\n\nEvery Otoroshi instance provides its own embedded OpenAPI descriptor at :\n\nhttp://otoroshi.oto.tools:8080/api/openapi.json\n\n## Swagger documentation\n\nYou can read the OpenAPI descriptor in a more human friendly fashion using `Swagger UI`. The swagger UI documentation of the Otoroshi admin API is available at :\n\nhttps://maif.github.io/otoroshi/swagger-ui/index.html\n\nEvery Otoroshi instance provides its own embedded OpenAPI descriptor at :\n\nhttp://otoroshi.oto.tools:8080/api/swagger/ui\n\nYou can also read the swagger UI documentation of the Otoroshi admin API below :\n\n@@@ div { .swagger-frame }\n\n\n@@@\n"
  },
  {
    "name": "architecture.md",
    "id": "/architecture.md",
    "url": "/architecture.html",
    "title": "Architecture",
    "content": "# Architecture\n\nWhen we started the development of Otoroshi, we had several classical patterns in mind like `Service gateway`, `Service locator`, `Circuit breakers`, etc ...\n\nAt start we thought about providing a bunch of librairies that would be included in each microservice or app to perform these tasks. But the more we were thinking about it, the more it was feeling weird, unagile, etc, it also prevented us to use any technical stack we wanted to use. So we decided to change our approach to something more universal.\n\nWe chose to make Otoroshi the central part of our microservices system, something between a reverse-proxy, a service gateway and a service locator where each call to a microservice (even from another microservice) must pass through Otoroshi. There are multiple benefits to do that, each call can be logged, audited, monitored, integrated with a circuit breaker, etc without imposing libraries and technical stack. Any service is exposed through its own domain and we rely only on DNS to handle the service location part. Any access to a service is secured by default with an api key and is supervised by a circuit breaker to avoid cascading failures.\n\n@@@ div { .centered-img }\n<img src=\"./img/architecture-1-bis.png\" />\n@@@\n\nOtoroshi tries to embrace our @ref:[global philosophy](./about.md#philosophy) by providing a full featured REST admin api, a gorgeous admin dashboard written in [React](https://reactjs.org/) that uses the api, by generating traffic events, alerts events, audit events that can be consumed by several channels. Otoroshi also supports a bunch of datastores to better match with different use cases.\n\n@@@ div { .centered-img }\n<img src=\"./img/architecture-2-bis.png\" />\n@@@\n"
  },
  {
    "name": "aws.md",
    "id": "/deploy/aws.md",
    "url": "/deploy/aws.html",
    "title": "AWS - Elastic Beanstalk",
    "content": "# AWS - Elastic Beanstalk\n\nNow you want to use Otoroshi on AWS. There are multiple options to deploy Otoroshi on AWS, \nfor instance :\n\n* You can deploy the @ref:[Docker image](../install/get-otoroshi.md#from-docker) on [Amazon ECS](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html)\n* You can create a basic [Amazon EC2](https://docs.aws.amazon.com/fr_fr/AWSEC2/latest/UserGuide/concepts.html), access it via SSH, then \ndeploy the @ref:[otoroshi.jar](../install/get-otoroshi.md#from-jar-file)     \n* Or you can use [AWS Elastic Beanstalk](https://aws.amazon.com/fr/elasticbeanstalk)\n\nIn this section we are going to cover how to deploy Otoroshi on [AWS Elastic Beanstalk](https://aws.amazon.com/fr/elasticbeanstalk). \n\n## AWS Elastic Beanstalk Overview\nUnlike Clever Cloud, to deploy an application on AWS Elastic Beanstalk, you don't link your app to your VCS repository, push your code and expect it to be built and run.\n\nAWS Elastic Beanstalk does only the run part. So you have to handle your own build pipeline, upload a Zip file containing your runnable, then AWS Elastic Beanstalk will take it from there.  \n  \nEg: for apps running on the JVM (Scala/Java/Kotlin) a Zip with the jar inside would suffice, for apps running in a Docker container, a Zip with the DockerFile would be enough.   \n\n\n## Prepare your deployment target\nActually, there are 2 options to build your target. \n\nEither you create a DockerFile from this @ref:[Docker image](../install/get-otoroshi.md#from-docker), build a zip, and do all the Otoroshi custom configuration using ENVs.\n\nOr you download the @ref:[otoroshi.jar](../install/get-otoroshi.md#from-jar-file), do all the Otoroshi custom configuration using your own otoroshi.conf, and create a DockerFile that runs the jar using your otoroshi.conf. \n\nFor the second option your DockerFile would look like this :\n\n```dockerfile\nFROM openjdk:8\nVOLUME /tmp\nEXPOSE 8080\nADD otoroshi.jar otoroshi.jar\nADD otoroshi.conf otoroshi.conf\nRUN sh -c 'touch /otoroshi.jar'\nENV JAVA_OPTS=\"\"\nENTRYPOINT [ \"sh\", \"-c\", \"java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -Dconfig.file=/otoroshi.conf -jar /otoroshi.jar\" ]\n``` \n \nI'd recommend the second option.\n       \nNow Zip your target (Jar + Conf + DockerFile) and get ready for deployment.     \n\n## Create an Otoroshi instance on AWS Elastic Beanstalk\nFirst, go to [AWS Elastic Beanstalk Console](https://eu-west-3.console.aws.amazon.com/elasticbeanstalk/home?region=eu-west-3#/welcome), don't forget to sign in and make sure that you are in the good region (eg : eu-west-3 for Paris).\n\nHit **Get started** \n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-0.png\" />\n@@@\n\nSpecify the **Application name** of your application, Otoroshi for example.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-1.png\" />\n@@@\n \nChoose the **Platform** of the application you want to create, in your case use Docker.\n\nFor **Application code** choose **Upload your code** then hit **Upload**.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-2.png\" />\n@@@\n\nBrowse the zip created in the [previous section](#prepare-your-deployment-target) from your machine. \n\nAs you can see in the image above, you can also choose an S3 location, you can imagine that at the end of your build pipeline you upload your Zip to S3, and then get it from there (I wouldn't recommend that though).\n  \nWhen the upload is done, hit **Configure more options**.\n   \n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-3.png\" />\n@@@ \n \nRight now an AWS Elastic Beanstalk application has been created, and by default an environment named Otoroshi-env is being created as well.\n\nAWS Elastic Beanstalk can manage multiple environments of the same application, for instance environments can be (prod, preprod, expriments...).  \n\nOtoroshi is a bit particular, it doesn't make much sense to have multiple environments, since Otoroshi will handle all the requests from/to downstream services regardless of the environment.        \n \nAs you see in the image above, we are now configuring the Otoroshi-env, the one and only environment of Otoroshi.\n  \nFor **Configuration presets**, choose custom configuration, now you have a load balancer for your environment with the capacity of at least one instance and at most four.\nI'd recommend at least 2 instances, to change that, on the **Capacity** card hit **Modify**.         \n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-4.png\" />\n@@@\n\nChange the **Instances** to min 2, max 4 then hit **Save**. For the **Scaling triggers**, I'd keep the default values, but know that you can edit the capacity config any time you want, it only costs a redeploy, which will be done automatically by the way.\n       \nInstances size is by default t2.micro, which is a bit small for running Otoroshi, I'd recommend a t2.medium.     \nOn the **Instances** card hit **Modify**.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-5.png\" />\n@@@\n\nFor **Instance type** choose t2.medium, then hit **Save**, no need to change the volume size, unless you have a lot of http call faults, which means a lot more logs, in that case the default volume size may not be enough.\n\nThe default environment created for Otoroshi, for instance Otoroshi-env, is a web server environment which fits in your case, but the thing is that on AWS Elastic Beanstalk by default a web server environment for a docker-based application, runs behind an Nginx proxy.\nWe have to remove that proxy. So on the **Software** card hit **Modify**.\n        \n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-6.png\" />\n@@@        \n    \nFor **Proxy server** choose None then hit **Save**.\n\nAlso note that you can set Envs for Otoroshi in same page (see image below). \n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-7.png\" />\n@@@  \n\nTo finalise the creation process, hit **Create app** on the bottom right.\n\nThe Otoroshi app is now created, and it's running which is cool, but we still don't have neither a **datastore** nor **https**.\n  \n## Create an Otoroshi datastore on AWS ElastiCache\n\nBy default Otoroshi uses non persistent memory to store it's data, Otoroshi supports many kinds of datastores. In this section we will be covering Redis datastore.   \n\nBefore starting, using a datastore hosted by AWS is not at all mandatory, feel free to use your own if you like, but if you want to learn more about ElastiCache, this section may interest you, otherwise you can skip it.\n\nGo to [AWS ElastiCache](https://eu-west-3.console.aws.amazon.com/elasticache/home?region=eu-west-3#) and hit **Get Started Now**.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-8.png\" />\n@@@  \n\nFor **Cluster engine** keep Redis.\n\nChoose a **Name** for your datastore, for instance otoroshi-datastore.\n\nYou can keep all the other default values and hit **Create** on the bottom right of the page.\n\nOnce your Redis Cluster is created, it would look like the image below.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-9.png\" />\n@@@  \n\n\nFor applications in the same security group as your cluster, redis cluster is accessible via the **Primary Endpoint**. Don't worry the default security group is fine, you don't need any configuration to access the cluster from Otoroshi.\n\nTo make Otoroshi use the created cluster, you can either use Envs `APP_STORAGE=redis`, `REDIS_HOST` and `REDIS_PORT`, or set `app.storage=redis`, `app.redis.host` and `app.redis.port` in your otoroshi.conf.\n\n## Create SSL certificate and configure your domain\n\nOtoroshi has now a datastore, but not yet ready for use. \n\nIn order to get it ready you need to :\n\n* Configure Otoroshi with your domain \n* Create a wildcard SSL certificate for your domain\n* Configure Otoroshi AWS Elastic Beanstalk instance with the SSL certificate \n* Configure your DNS to redirect all traffic on your domain to Otoroshi  \n  \n### Configure Otoroshi with your domain\n\nYou can use ENVs or you can use a custom otoroshi.conf in your Docker container.\n\nFor the second option your otoroshi.conf would look like this :\n\n``` \n   include \"application.conf\"\n   http.port = 8080\n   app {\n     env = \"prod\"\n     domain = \"mysubdomain.oto.tools\"\n     rootScheme = \"https\"\n     snowflake {\n       seed = 0\n     }\n     events {\n       maxSize = 1000\n     }\n     backoffice {\n       subdomain = \"otoroshi\"\n       session {\n         exp = 86400000\n       }\n     }\n     \n     storage = \"redis\"\n     redis {\n        host=\"myredishost\"\n        port=myredisport\n     }\n   \n     privateapps {\n       subdomain = \"privateapps\"\n     }\n   \n     adminapi {\n       targetSubdomain = \"otoroshi-admin-internal-api\"\n       exposedSubdomain = \"otoroshi-api\"\n       defaultValues {\n         backOfficeGroupId = \"admin-api-group\"\n         backOfficeApiKeyClientId = \"admin-client-id\"\n         backOfficeApiKeyClientSecret = \"admin-client-secret\"\n         backOfficeServiceId = \"admin-api-service\"\n       }\n       proxy {\n         https = true\n         local = false\n       }\n     }\n     claim {\n       sharedKey = \"myclaimsharedkey\"\n     }\n   }\n   \n   play.http {\n     session {\n       secure = false\n       httpOnly = true\n       maxAge = 2147483646\n       domain = \".mysubdomain.oto.tools\"\n       cookieName = \"oto-sess\"\n     }\n   }\n``` \n\n### Create a wildcard SSL certificate for your domain\n\nGo to [AWS Certificate Manager](https://eu-west-3.console.aws.amazon.com/acm/home?region=eu-west-3#/firstrun).\n\nBelow **Provision certificates** hit **Get started**.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-10.png\" />\n@@@   \n \nKeep the default selected value **Request a public certificate** and hit **Request a certificate**.\n \n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-11.png\" />\n@@@  \n\nPut your **Domain name**, use *. for wildcard, for instance *\\*.mysubdomain.oto.tools*, then hit **Next**.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-12.png\" />\n@@@  \n\nYou can choose between **Email validation** and **DNS validation**, I'd recommend **DNS validation**, then hit **Review**.    \n    \n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-13.png\" />\n@@@ \n \nVerify that you did put the right **Domain name** then hit **Confirm and request**.   \n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-14.png\" />\n@@@\n \nAs you see in the image above, to let Amazon do the validation you have to add the `CNAME` record to your DNS configuration. Normally this operation takes around one day.\n  \n### Configure Otoroshi AWS Elastic Beanstalk instance with the SSL certificate \n\nOnce the certificate is validated, you need to modify the configuration of Otoroshi-env to add the SSL certificate for HTTPS. \nFor that you need to go to [AWS Elastic Beanstalk applications](https://eu-west-3.console.aws.amazon.com/elasticbeanstalk/home?region=eu-west-3#/applications),\nhit **Otoroshi-env**, then on the left side hit **Configuration**, then on the **Load balancer** card hit **Modify**.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-15.png\" />\n@@@\n\nIn the **Application Load Balancer** section hit **Add listener**.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-16.png\" />\n@@@\n\nFill the popup as the image above, then hit **Add**.   \n\nYou should now be seeing something like this : \n   \n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-17.png\" />\n@@@   \n \n \nMake sure that your listener is enabled, and on the bottom right of the page hit **Apply**.\n\nNow you have **https**, so let's use Otoroshi.\n\n### Configure your DNS to redirect all traffic on your domain to Otoroshi\n  \nIt's actually pretty simple, you just need to add a `CNAME` record to your DNS configuration, that redirects *\\*.mysubdomain.oto.tools* to the DNS name of Otoroshi's load balancer.\n\nTo find the DNS name of Otoroshi's load balancer go to [AWS Ec2](https://eu-west-3.console.aws.amazon.com/ec2/v2/home?region=eu-west-3#LoadBalancers:tag:elasticbeanstalk:environment-name=Otoroshi-env;sort=loadBalancerName)\n\nYou would find something like this : \n  \n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-elb-18.png\" />\n@@@   \n\nThere is your DNS name, so add your `CNAME` record. \n \nOnce all these steps are done, the AWS Elastic Beanstalk Otoroshi instance, would now be handling all the requests on your domain. ;)    \n"
  },
  {
    "name": "clever-cloud.md",
    "id": "/deploy/clever-cloud.md",
    "url": "/deploy/clever-cloud.html",
    "title": "Clever-Cloud",
    "content": "# Clever-Cloud\n\nNow you want to use Otoroshi on Clever Cloud. Otoroshi has been designed and created to run on Clever Cloud and a lot of choices were made because of how Clever Cloud works.\n\n## Create an Otoroshi instance on CleverCloud\n\nIf you want to customize the configuration @ref:[use env. variables](../install/setup-otoroshi.md#environnement-variables), you can use [the example provided below](#example-of-clevercloud-env-variables)\n\nCreate a new CleverCloud app based on a clevercloud git repo (not empty) or a github project of your own (not empty).\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-cc-jar-0.png\" />\n@@@\n\nThen choose what kind of app your want to create, for Otoroshi, choose `Java + Jar`\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-cc-jar-1.png\" />\n@@@\n\nNext, set up choose instance size and auto-scalling. Otoroshi can run on small instances, especially if you just want to test it.\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-cc-2.png\" />\n@@@\n\nFinally, choose a name for your app\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-cc-3.png\" />\n@@@\n\nNow you just need to customize environnment variables\n\nat this point, you can also add other env. variables to configure Otoroshi like in [the example provided below](#example-of-clevercloud-env-variables)\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-cc-4-bis.png\" />\n@@@\n\nYou can also use expert mode :\n\n@@@ div { .centered-img }\n<img src=\"../imgs/deploy-cc-4.png\" />\n@@@\n\nNow, your app is ready, don't forget to add a custom domains name on the CleverCloud app matching the Otoroshi app domain. \n\n## Example of CleverCloud env. variables\n\nYou can add more env variables to customize your Otoroshi instance like the following. Use the expert mode to copy/paste all the values in one shot. If you want an real datastore, create a redis addon on clevercloud, link it to your otoroshi app and change the `APP_STORAGE` variable to `redis`\n\n<div id=\"clevercloud-envvars\"></div>\n\n<div class=\"hide\">\n```\nADMIN_API_CLIENT_ID=xxxx\nADMIN_API_CLIENT_SECRET=xxxxx\nADMIN_API_GROUP=xxxxxx\nADMIN_API_SERVICE_ID=xxxxxxx\nCLAIM_SHAREDKEY=xxxxxxx\nOTOROSHI_INITIAL_ADMIN_LOGIN=youremailaddress\nOTOROSHI_INITIAL_ADMIN_PASSWORD=yourpassword\nPLAY_CRYPTO_SECRET=xxxxxx\nSESSION_NAME=oto-session\nAPP_DOMAIN=yourdomain.tech\nAPP_ENV=prod\nAPP_STORAGE=inmemory\nAPP_ROOT_SCHEME=https\nCC_PRE_BUILD_HOOK=curl -L -o otoroshi.jar 'https://github.com/MAIF/otoroshi/releases/download/${latest_otoroshi_version}/otoroshi.jar'\nCC_JAR_PATH=./otoroshi.jar\nCC_JAVA_VERSION=11\nPORT=8080\nSESSION_DOMAIN=.yourdomain.tech\nSESSION_MAX_AGE=604800000\nSESSION_SECURE_ONLY=true\nUSER_AGENT=otoroshi\nMAX_EVENTS_SIZE=1\nWEBHOOK_SIZE=100\nAPP_BACKOFFICE_SESSION_EXP=86400000\nAPP_PRIVATEAPPS_SESSION_EXP=86400000\nENABLE_METRICS=true\nOTOROSHI_ANALYTICS_PRESSURE_ENABLED=true\nUSE_CACHE=true\n```\n</div>"
  },
  {
    "name": "clustering.md",
    "id": "/deploy/clustering.md",
    "url": "/deploy/clustering.html",
    "title": "Otoroshi clustering",
    "content": "# Otoroshi clustering\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "index.md",
    "id": "/deploy/index.md",
    "url": "/deploy/index.html",
    "title": "Deploy to production",
    "content": "# Deploy to production\n\nNow it's time to deploy Otoroshi in production, in this chapter we will see what kind of things you can do.\n\n* @ref:[Clustering](./clustering.md)\n* @ref:[Kubernetes](./kubernetes.md)\n* @ref:[Clever Cloud](./clever-cloud.md)\n* @ref:[AWS - Elastic Beanstalk](./aws.md)\n* @ref:[others](./other.md)  \n* @ref:[Scaling](./scaling.md)  \n\n@@@ index\n\n* [Clustering](./clustering.md)\n* [Kubernetes](./kubernetes.md)\n* [Clever Cloud](./clever-cloud.md)\n* [AWS - Elastic Beanstalk](./aws.md)\n* [others](./other.md)  \n* [Scaling](./scaling.md)  \n\n@@@"
  },
  {
    "name": "kubernetes.md",
    "id": "/deploy/kubernetes.md",
    "url": "/deploy/kubernetes.html",
    "title": "Kubernetes",
    "content": "# Kubernetes\n\nStarting at version 1.5.0, Otoroshi provides a native Kubernetes support. Multiple otoroshi jobs (that are actually kubernetes controllers) are provided in order to\n\n- sync kubernetes secrets of type `kubernetes.io/tls` to otoroshi certificates\n- act as a standard ingress controller (supporting `Ingress` objects)\n- provide Custom Resource Definitions (CRDs) to manage Otoroshi entities from Kubernetes and act as an ingress controller with its own resources\n\n## Installing otoroshi on your kubernetes cluster\n\n@@@ warning\nYou need to have cluster admin privileges to install otoroshi and its service account, role mapping and CRDs on a kubernetes cluster. We also advise you to create a dedicated namespace (you can name it `otoroshi` for example) to install otoroshi\n@@@\n\nIf you want to deploy otoroshi into your kubernetes cluster, you can download the deployment descriptors from https://github.com/MAIF/otoroshi/tree/master/kubernetes and use kustomize to create your own overlay.\n\nYou can also create a `kustomization.yaml` file with a remote base\n\n```yaml\nbases:\n- github.com/MAIF/otoroshi/kubernetes/kustomize/overlays/simple/?ref=v1.5.0-dev\n```\n\nThen deploy it with `kubectl apply -k ./overlays/myoverlay`. \n\nYou can also use Helm to deploy a simple otoroshi cluster on your kubernetes cluster\n\n```sh\nhelm repo add otoroshi https://maif.github.io/otoroshi/helm\nhelm install my-otoroshi otoroshi/otoroshi\n```\n\nBelow, you will find example of deployment. Do not hesitate to adapt them to your needs. Those descriptors have value placeholders that you will need to replace with actual values like \n\n```yaml\n env:\n  - name: APP_STORAGE_ROOT\n    value: otoroshi\n  - name: APP_DOMAIN\n    value: ${domain}\n```\n\nyou will have to edit it to make it look like\n\n```yaml\n env:\n  - name: APP_STORAGE_ROOT\n    value: otoroshi\n  - name: APP_DOMAIN\n    value: 'apis.my.domain'\n```\n\nif you don't want to use placeholders and environment variables, you can create a secret containing the configuration file of otoroshi\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: otoroshi-config\ntype: Opaque\nstringData:\n  oto.conf: >\n    include \"application.conf\"\n    app {\n      storage = \"redis\"\n      domain = \"apis.my.domain\"\n    }\n```\n\nand mount it in the otoroshi container\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: otoroshi-deployment\nspec:\n  selector:\n    matchLabels:\n      run: otoroshi-deployment\n  template:\n    metadata:\n      labels:\n        run: otoroshi-deployment\n    spec:\n      serviceAccountName: otoroshi-admin-user\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: maif/otoroshi:1.5.0-dev-jdk11\n        imagePullPolicy: IfNotPresent\n        name: otoroshi\n        args: ['-Dconfig.file=/usr/app/otoroshi/conf/oto.conf']\n        ports:\n          - containerPort: 8080\n            name: \"http\"\n            protocol: TCP\n          - containerPort: 8443\n            name: \"https\"\n            protocol: TCP\n        volumeMounts:\n        - name: otoroshi-config\n          mountPath: \"/usr/app/otoroshi/conf\"\n          readOnly: true\n      volumes:\n      - name: otoroshi-config\n        secret:\n          secretName: otoroshi-config\n        ...\n```\n\nYou can also create several secrets for each placeholder, mount them to the otoroshi container then use their file path as value\n\n```yaml\n env:\n  - name: APP_STORAGE_ROOT\n    value: otoroshi\n  - name: APP_DOMAIN\n    value: 'file:///the/path/of/the/secret/file'\n```\n\nyou can use the same trick in the config. file itself\n\n### Note on bare metal kubernetes cluster installation\n\n@@@ note\nBare metal kubernetes clusters don't come with support for external loadbalancers (service of type `LoadBalancer`). So you will have to provide this feature in order to route external TCP traffic to Otoroshi containers running inside the kubernetes cluster. You can use projects like [MetalLB](https://metallb.universe.tf/) that provide software `LoadBalancer` services to bare metal clusters or you can use and customize examples below.\n@@@\n\n@@@ warning\nWe don't recommand running Otoroshi behind an existing ingress controller (or something like that) as you will not be able to use features like TCP proxying, TLS, mTLS, etc. Also, this additional layer of reverse proxy will increase call latencies.\n@@@\n\n### Common manifests\n\nthe following manifests are always needed. They create otoroshi CRDs, tokens, role, etc. Redis deployment is not mandatory, it's just an example. You can use your own existing setup.\n\nrbac.yaml\n:   @@snip [rbac.yaml](../snippets/kubernetes/kustomize/base/rbac.yaml) \n\ncrds.yaml\n:   @@snip [crds.yaml](../snippets/kubernetes/kustomize/base/crds.yaml) \n\nredis.yaml\n:   @@snip [redis.yaml](../snippets/kubernetes/kustomize/base/redis.yaml) \n\n\n### Deploy a simple otoroshi instanciation on a cloud provider managed kubernetes cluster\n\nHere we have 2 replicas connected to the same redis instance. Nothing fancy. We use a service of type `LoadBalancer` to expose otoroshi to the rest of the world. You have to setup your DNS to bind otoroshi domain names to the `LoadBalancer` external `CNAME` (see the example below)\n\ndeployment.yaml\n:   @@snip [deployment.yaml](../snippets/kubernetes/kustomize/overlays/simple/deployment.yaml) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/simple/dns.example) \n\n### Deploy a simple otoroshi instanciation on a bare metal kubernetes cluster\n\nHere we have 2 replicas connected to the same redis instance. Nothing fancy. The otoroshi instance are exposed as `nodePort` so you'll have to add a loadbalancer in front of your kubernetes nodes to route external traffic (TCP) to your otoroshi instances. You have to setup your DNS to bind otoroshi domain names to your loadbalancer (see the example below). \n\ndeployment.yaml\n:   @@snip [deployment.yaml](../snippets/kubernetes/kustomize/overlays/simple-baremetal/deployment.yaml) \n\nhaproxy.example\n:   @@snip [haproxy.example](../snippets/kubernetes/kustomize/overlays/simple-baremetal/haproxy.example) \n\nnginx.example\n:   @@snip [nginx.example](../snippets/kubernetes/kustomize/overlays/simple-baremetal/nginx.example) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/simple-baremetal/dns.example) \n\n\n### Deploy a simple otoroshi instanciation on a bare metal kubernetes cluster using a DaemonSet\n\nHere we have one otoroshi instance on each kubernetes node (with the `otoroshi-kind: instance` label) with redis persistance. The otoroshi instances are exposed as `hostPort` so you'll have to add a loadbalancer in front of your kubernetes nodes to route external traffic (TCP) to your otoroshi instances. You have to setup your DNS to bind otoroshi domain names to your loadbalancer (see the example below). \n\ndeployment.yaml\n:   @@snip [deployment.yaml](../snippets/kubernetes/kustomize/overlays/simple-baremetal-daemonset/deployment.yaml) \n\nhaproxy.example\n:   @@snip [haproxy.example](../snippets/kubernetes/kustomize/overlays/simple-baremetal-daemonset/haproxy.example) \n\nnginx.example\n:   @@snip [nginx.example](../snippets/kubernetes/kustomize/overlays/simple-baremetal-daemonset/nginx.example) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/simple-baremetal-daemonset/dns.example) \n\n### Deploy an otoroshi cluster on a cloud provider managed kubernetes cluster\n\nHere we have 2 replicas of an otoroshi leader connected to a redis instance and 2 replicas of an otoroshi worker connected to the leader. We use a service of type `LoadBalancer` to expose otoroshi leader/worker to the rest of the world. You have to setup your DNS to bind otoroshi domain names to the `LoadBalancer` external `CNAME` (see the example below)\n\ndeployment.yaml\n:   @@snip [deployment.yaml](../snippets/kubernetes/kustomize/overlays/cluster/deployment.yaml) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/cluster/dns.example) \n\n### Deploy an otoroshi cluster on a bare metal kubernetes cluster\n\nHere we have 2 replicas of otoroshi leader connected to the same redis instance and 2 replicas for otoroshi worker. The otoroshi instances are exposed as `nodePort` so you'll have to add a loadbalancer in front of your kubernetes nodes to route external traffic (TCP) to your otoroshi instances. You have to setup your DNS to bind otoroshi domain names to your loadbalancer (see the example below). \n\ndeployment.yaml\n:   @@snip [deployment.yaml](../snippets/kubernetes/kustomize/overlays/cluster-baremetal/deployment.yaml) \n\nnginx.example\n:   @@snip [nginx.example](../snippets/kubernetes/kustomize/overlays/cluster-baremetal/nginx.example) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/cluster-baremetal/dns.example) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/cluster-baremetal/dns.example) \n\n### Deploy an otoroshi cluster on a bare metal kubernetes cluster using DaemonSet\n\nHere we have 1 otoroshi leader instance on each kubernetes node (with the `otoroshi-kind: leader` label) connected to the same redis instance and 1 otoroshi worker instance on each kubernetes node (with the `otoroshi-kind: worker` label). The otoroshi instances are exposed as `nodePort` so you'll have to add a loadbalancer in front of your kubernetes nodes to route external traffic (TCP) to your otoroshi instances. You have to setup your DNS to bind otoroshi domain names to your loadbalancer (see the example below). \n\ndeployment.yaml\n:   @@snip [deployment.yaml](../snippets/kubernetes/kustomize/overlays/cluster-baremetal-daemonset/deployment.yaml) \n\nnginx.example\n:   @@snip [nginx.example](../snippets/kubernetes/kustomize/overlays/cluster-baremetal-daemonset/nginx.example) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/cluster-baremetal-daemonset/dns.example) \n\ndns.example\n:   @@snip [dns.example](../snippets/kubernetes/kustomize/overlays/cluster-baremetal-daemonset/dns.example) \n\n## Using Otoroshi as an Ingress Controller\n\nIf you want to use Otoroshi as an [Ingress Controller](https://kubernetes.io/fr/docs/concepts/services-networking/ingress/), just go to the danger zone, and in `Global scripts` add the job named `Kubernetes Ingress Controller`.\n\nThen add the following configuration for the job (with your own tweaks of course)\n\n```json\n{\n  \"KubernetesConfig\": {\n    \"enabled\": true,\n    \"endpoint\": \"https://127.0.0.1:6443\",\n    \"token\": \"eyJhbGciOiJSUzI....F463SrpOehQRaQ\",\n    \"namespaces\": [\n      \"*\"\n    ]\n  }\n}\n```\n\nthe configuration can have the following values \n\n```javascript\n{\n  \"KubernetesConfig\": {\n    \"endpoint\": \"https://127.0.0.1:6443\", // the endpoint to talk to the kubernetes api, optional\n    \"token\": \"xxxx\", // the bearer token to talk to the kubernetes api, optional\n    \"userPassword\": \"user:password\", // the user password tuple to talk to the kubernetes api, optional\n    \"caCert\": \"/etc/ca.cert\", // the ca cert file path to talk to the kubernetes api, optional\n    \"trust\": false, // trust any cert to talk to the kubernetes api, optional\n    \"namespaces\": [\"*\"], // the watched namespaces\n    \"labels\": [\"label\"], // the watched namespaces\n    \"ingressClasses\": [\"otoroshi\"], // the watched kubernetes.io/ingress.class annotations, can be *\n    \"defaultGroup\": \"default\", // the group to put services in otoroshi\n    \"ingresses\": true, // sync ingresses\n    \"crds\": false, // sync crds\n    \"kubeLeader\": false, // delegate leader election to kubernetes, to know where the sync job should run\n    \"restartDependantDeployments\": true, // when a secret/cert changes from otoroshi sync, restart dependant deployments\n    \"templates\": { // template for entities that will be merged with kubernetes entities. can be \"default\" to use otoroshi default templates\n      \"service-group\": {},\n      \"service-descriptor\": {},\n      \"apikeys\": {},\n      \"global-config\": {},\n      \"jwt-verifier\": {},\n      \"tcp-service\": {},\n      \"certificate\": {},\n      \"auth-module\": {},\n      \"data-exporter\": {},\n      \"script\": {},\n      \"organization\": {},\n      \"team\": {},\n      \"data-exporter\": {}\n    }\n  }\n}\n```\n\nIf `endpoint` is not defined, Otoroshi will try to get it from `$KUBERNETES_SERVICE_HOST` and `$KUBERNETES_SERVICE_PORT`.\nIf `token` is not defined, Otoroshi will try to get it from the file at `/var/run/secrets/kubernetes.io/serviceaccount/token`.\nIf `caCert` is not defined, Otoroshi will try to get it from the file at `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt`.\nIf `$KUBECONFIG` is defined, `endpoint`, `token` and `caCert` will be read from the current context of the file referenced by it.\n\nNow you can deploy your first service ;)\n\n### Deploy an ingress route\n\nnow let's say you want to deploy an http service and route to the outside world through otoroshi\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: http-app-deployment\nspec:\n  selector:\n    matchLabels:\n      run: http-app-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: http-app-deployment\n    spec:\n      containers:\n      - image: kennethreitz/httpbin\n        imagePullPolicy: IfNotPresent\n        name: otoroshi\n        ports:\n          - containerPort: 80\n            name: \"http\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: http-app-service\nspec:\n  ports:\n    - port: 8080\n      targetPort: http\n      name: http\n  selector:\n    run: http-app-deployment\n---\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: http-app-ingress\n  annotations:\n    kubernetes.io/ingress.class: otoroshi\nspec:\n  tls:\n  - hosts:\n    - httpapp.foo.bar\n    secretName: http-app-cert\n  rules:\n  - host: httpapp.foo.bar\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: http-app-service\n          servicePort: 8080\n```\n\nonce deployed, otoroshi will sync with kubernetes and create the corresponding service to route your app. You will be able to access your app with\n\n```sh\ncurl -X GET https://httpapp.foo.bar/get\n```\n\n### Support for Ingress Classes\n\nSince Kubernetes 1.18, you can use `IngressClass` type of manifest to specify which ingress controller you want to use for a deployment (https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#extended-configuration-with-ingress-classes). Otoroshi is fully compatible with this new manifest `kind`. To use it, configure the Ingress job to match your controller\n\n```javascript\n{\n  \"KubernetesConfig\": {\n    ...\n    \"ingressClasses\": [\"otoroshi.io/ingress-controller\"],\n    ...\n  }\n}\n```\n\nthen you have to deploy an `IngressClass` to declare Otoroshi as an ingress controller\n\n```yaml\napiVersion: \"networking.k8s.io/v1beta1\"\nkind: \"IngressClass\"\nmetadata:\n  name: \"otoroshi-ingress-controller\"\nspec:\n  controller: \"otoroshi.io/ingress-controller\"\n  parameters:\n    apiGroup: \"proxy.otoroshi.io/v1alpha\"\n    kind: \"IngressParameters\"\n    name: \"otoroshi-ingress-controller\"\n```\n\nand use it in your `Ingress`\n\n```yaml\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: http-app-ingress\nspec:\n  ingressClassName: otoroshi-ingress-controller\n  tls:\n  - hosts:\n    - httpapp.foo.bar\n    secretName: http-app-cert\n  rules:\n  - host: httpapp.foo.bar\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: http-app-service\n          servicePort: 8080\n```\n\n### Use multiple ingress controllers\n\nIt is of course possible to use multiple ingress controller at the same time (https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#using-multiple-ingress-controllers) using the annotation `kubernetes.io/ingress.class`. By default, otoroshi reacts to the class `otoroshi`, but you can make it the default ingress controller with the following config\n\n```json\n{\n  \"KubernetesConfig\": {\n    ...\n    \"ingressClass\": \"*\",\n    ...\n  }\n}\n```\n\n### Supported annotations\n\nif you need to customize the service descriptor behind an ingress rule, you can use some annotations. If you need better customisation, just go to the CRDs part. The following annotations are supported :\n\n- `ingress.otoroshi.io/groups`\n- `ingress.otoroshi.io/group`\n- `ingress.otoroshi.io/groupId`\n- `ingress.otoroshi.io/name`\n- `ingress.otoroshi.io/targetsLoadBalancing`\n- `ingress.otoroshi.io/stripPath`\n- `ingress.otoroshi.io/enabled`\n- `ingress.otoroshi.io/userFacing`\n- `ingress.otoroshi.io/privateApp`\n- `ingress.otoroshi.io/forceHttps`\n- `ingress.otoroshi.io/maintenanceMode`\n- `ingress.otoroshi.io/buildMode`\n- `ingress.otoroshi.io/strictlyPrivate`\n- `ingress.otoroshi.io/sendOtoroshiHeadersBack`\n- `ingress.otoroshi.io/readOnly`\n- `ingress.otoroshi.io/xForwardedHeaders`\n- `ingress.otoroshi.io/overrideHost`\n- `ingress.otoroshi.io/allowHttp10`\n- `ingress.otoroshi.io/logAnalyticsOnServer`\n- `ingress.otoroshi.io/useAkkaHttpClient`\n- `ingress.otoroshi.io/useNewWSClient`\n- `ingress.otoroshi.io/tcpUdpTunneling`\n- `ingress.otoroshi.io/detectApiKeySooner`\n- `ingress.otoroshi.io/letsEncrypt`\n- `ingress.otoroshi.io/publicPatterns`\n- `ingress.otoroshi.io/privatePatterns`\n- `ingress.otoroshi.io/additionalHeaders`\n- `ingress.otoroshi.io/additionalHeadersOut`\n- `ingress.otoroshi.io/missingOnlyHeadersIn`\n- `ingress.otoroshi.io/missingOnlyHeadersOut`\n- `ingress.otoroshi.io/removeHeadersIn`\n- `ingress.otoroshi.io/removeHeadersOut`\n- `ingress.otoroshi.io/headersVerification`\n- `ingress.otoroshi.io/matchingHeaders`\n- `ingress.otoroshi.io/ipFiltering.whitelist`\n- `ingress.otoroshi.io/ipFiltering.blacklist`\n- `ingress.otoroshi.io/api.exposeApi`\n- `ingress.otoroshi.io/api.openApiDescriptorUrl`\n- `ingress.otoroshi.io/healthCheck.enabled`\n- `ingress.otoroshi.io/healthCheck.url`\n- `ingress.otoroshi.io/jwtVerifier.ids`\n- `ingress.otoroshi.io/jwtVerifier.enabled`\n- `ingress.otoroshi.io/jwtVerifier.excludedPatterns`\n- `ingress.otoroshi.io/authConfigRef`\n- `ingress.otoroshi.io/redirection.enabled`\n- `ingress.otoroshi.io/redirection.code`\n- `ingress.otoroshi.io/redirection.to`\n- `ingress.otoroshi.io/clientValidatorRef`\n- `ingress.otoroshi.io/transformerRefs`\n- `ingress.otoroshi.io/transformerConfig`\n- `ingress.otoroshi.io/accessValidator.enabled`\n- `ingress.otoroshi.io/accessValidator.excludedPatterns`\n- `ingress.otoroshi.io/accessValidator.refs`\n- `ingress.otoroshi.io/accessValidator.config`\n- `ingress.otoroshi.io/preRouting.enabled`\n- `ingress.otoroshi.io/preRouting.excludedPatterns`\n- `ingress.otoroshi.io/preRouting.refs`\n- `ingress.otoroshi.io/preRouting.config`\n- `ingress.otoroshi.io/issueCert`\n- `ingress.otoroshi.io/issueCertCA`\n- `ingress.otoroshi.io/gzip.enabled`\n- `ingress.otoroshi.io/gzip.excludedPatterns`\n- `ingress.otoroshi.io/gzip.whiteList`\n- `ingress.otoroshi.io/gzip.blackList`\n- `ingress.otoroshi.io/gzip.bufferSize`\n- `ingress.otoroshi.io/gzip.chunkedThreshold`\n- `ingress.otoroshi.io/gzip.compressionLevel`\n- `ingress.otoroshi.io/cors.enabled`\n- `ingress.otoroshi.io/cors.allowOrigin`\n- `ingress.otoroshi.io/cors.exposeHeaders`\n- `ingress.otoroshi.io/cors.allowHeaders`\n- `ingress.otoroshi.io/cors.allowMethods`\n- `ingress.otoroshi.io/cors.excludedPatterns`\n- `ingress.otoroshi.io/cors.maxAge`\n- `ingress.otoroshi.io/cors.allowCredentials`\n- `ingress.otoroshi.io/clientConfig.useCircuitBreaker`\n- `ingress.otoroshi.io/clientConfig.retries`\n- `ingress.otoroshi.io/clientConfig.maxErrors`\n- `ingress.otoroshi.io/clientConfig.retryInitialDelay`\n- `ingress.otoroshi.io/clientConfig.backoffFactor`\n- `ingress.otoroshi.io/clientConfig.connectionTimeout`\n- `ingress.otoroshi.io/clientConfig.idleTimeout`\n- `ingress.otoroshi.io/clientConfig.callAndStreamTimeout`\n- `ingress.otoroshi.io/clientConfig.callTimeout`\n- `ingress.otoroshi.io/clientConfig.globalTimeout`\n- `ingress.otoroshi.io/clientConfig.sampleInterval`\n- `ingress.otoroshi.io/enforceSecureCommunication`\n- `ingress.otoroshi.io/sendInfoToken`\n- `ingress.otoroshi.io/sendStateChallenge`\n- `ingress.otoroshi.io/secComHeaders.claimRequestName`\n- `ingress.otoroshi.io/secComHeaders.stateRequestName`\n- `ingress.otoroshi.io/secComHeaders.stateResponseName`\n- `ingress.otoroshi.io/secComTtl`\n- `ingress.otoroshi.io/secComVersion`\n- `ingress.otoroshi.io/secComInfoTokenVersion`\n- `ingress.otoroshi.io/secComExcludedPatterns`\n- `ingress.otoroshi.io/secComSettings.size`\n- `ingress.otoroshi.io/secComSettings.secret`\n- `ingress.otoroshi.io/secComSettings.base64`\n- `ingress.otoroshi.io/secComUseSameAlgo`\n- `ingress.otoroshi.io/secComAlgoChallengeOtoToBack.size`\n- `ingress.otoroshi.io/secComAlgoChallengeOtoToBack.secret`\n- `ingress.otoroshi.io/secComAlgoChallengeOtoToBack.base64`\n- `ingress.otoroshi.io/secComAlgoChallengeBackToOto.size`\n- `ingress.otoroshi.io/secComAlgoChallengeBackToOto.secret`\n- `ingress.otoroshi.io/secComAlgoChallengeBackToOto.base64`\n- `ingress.otoroshi.io/secComAlgoInfoToken.size`\n- `ingress.otoroshi.io/secComAlgoInfoToken.secret`\n- `ingress.otoroshi.io/secComAlgoInfoToken.base64`\n- `ingress.otoroshi.io/securityExcludedPatterns`\n\nfor more informations about it, just go to https://maif.github.io/otoroshi/swagger-ui/index.html\n\nwith the previous example, the ingress does not define any apikey, so the route is public. If you want to enable apikeys on it, you can deploy the following descriptor\n\n```yaml\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: http-app-ingress\n  annotations:\n    kubernetes.io/ingress.class: otoroshi\n    ingress.otoroshi.io/group: http-app-group\n    ingress.otoroshi.io/forceHttps: 'true'\n    ingress.otoroshi.io/sendOtoroshiHeadersBack: 'true'\n    ingress.otoroshi.io/overrideHost: 'true'\n    ingress.otoroshi.io/allowHttp10: 'false'\n    ingress.otoroshi.io/publicPatterns: ''\nspec:\n  tls:\n  - hosts:\n    - httpapp.foo.bar\n    secretName: http-app-cert\n  rules:\n  - host: httpapp.foo.bar\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: http-app-service\n          servicePort: 8080\n```\n\nnow you can use an existing apikey in the `http-app-group` to access your app\n\n```sh\ncurl -X GET https://httpapp.foo.bar/get -u existing-apikey-1:secret-1\n```\n\n## Use Otoroshi CRDs for a better/full integration\n\nOtoroshi provides some Custom Resource Definitions for kubernetes in order to manage Otoroshi related entities in kubernetes\n\n- `service-groups`\n- `service-descriptors`\n- `apikeys`\n- `certificates`\n- `global-configs`\n- `jwt-verifiers`\n- `auth-modules`\n- `scripts`\n- `tcp-services`\n- `data-exporters`\n- `admins`\n- `teams`\n- `organizations`\n\nusing CRDs, you will be able to deploy and manager those entities from kubectl or the kubernetes api like\n\n```sh\nsudo kubectl get apikeys --all-namespaces\nsudo kubectl get service-descriptors --all-namespaces\ncurl -X GET \\\n  -H 'Authorization: Bearer eyJhbGciOiJSUzI....F463SrpOehQRaQ' \\\n  -H 'Accept: application/json' -k \\\n  https://127.0.0.1:6443/apis/proxy.otoroshi.io/v1alpha1/apikeys | jq\n```\n\nYou can see this as better `Ingress` resources. Like any `Ingress` resource can define which controller it uses (using the `kubernetes.io/ingress.class` annotation), you can chose another kind of resource instead of `Ingress`. With Otoroshi CRDs you can even define resources like `Certificate`, `Apikey`, `AuthModules`, `JwtVerifier`, etc. It will help you to use all the power of Otoroshi while using the deployment model of kubernetes.\n \n@@@ warning\nwhen using Otoroshi CRDs, Kubernetes becomes the single source of truth for the synced entities. It means that any value in the descriptors deployed will overrides the one in Otoroshi datastore each time it's synced. So be careful if you use the Otoroshi UI or the API, some changes in configuration may be overriden by CRDs sync job.\n@@@\n\n### Resources examples\n\ngroup.yaml\n:   @@snip [group.yaml](../snippets/crds/group.yaml) \n\napikey.yaml\n:   @@snip [apikey.yaml](../snippets/crds/apikey.yaml) \n\nservice-descriptor.yaml\n:   @@snip [service.yaml](../snippets/crds/service-descriptor.yaml) \n\ncertificate.yaml\n:   @@snip [cert.yaml](../snippets/crds/certificate.yaml) \n\njwt.yaml\n:   @@snip [jwt.yaml](../snippets/crds/jwt.yaml) \n\nauth.yaml\n:   @@snip [auth.yaml](../snippets/crds/auth.yaml) \n\norganization.yaml\n:   @@snip [orga.yaml](../snippets/crds/organization.yaml) \n\nteam.yaml\n:   @@snip [team.yaml](../snippets/crds/team.yaml) \n\n\n### Configuration\n\nTo configure it, just go to the danger zone, and in `Global scripts` add the job named `Kubernetes Otoroshi CRDs Controller`. Then add the following configuration for the job (with your own tweak of course)\n\n```json\n{\n  \"KubernetesConfig\": {\n    \"enabled\": true,\n    \"crds\": true,\n    \"endpoint\": \"https://127.0.0.1:6443\",\n    \"token\": \"eyJhbGciOiJSUzI....F463SrpOehQRaQ\",\n    \"namespaces\": [\n      \"*\"\n    ]\n  }\n}\n```\n\nthe configuration can have the following values \n\n```javascript\n{\n  \"KubernetesConfig\": {\n    \"endpoint\": \"https://127.0.0.1:6443\", // the endpoint to talk to the kubernetes api, optional\n    \"token\": \"xxxx\", // the bearer token to talk to the kubernetes api, optional\n    \"userPassword\": \"user:password\", // the user password tuple to talk to the kubernetes api, optional\n    \"caCert\": \"/etc/ca.cert\", // the ca cert file path to talk to the kubernetes api, optional\n    \"trust\": false, // trust any cert to talk to the kubernetes api, optional\n    \"namespaces\": [\"*\"], // the watched namespaces\n    \"labels\": [\"label\"], // the watched namespaces\n    \"ingressClasses\": [\"otoroshi\"], // the watched kubernetes.io/ingress.class annotations, can be *\n    \"defaultGroup\": \"default\", // the group to put services in otoroshi\n    \"ingresses\": false, // sync ingresses\n    \"crds\": true, // sync crds\n    \"kubeLeader\": false, // delegate leader election to kubernetes, to know where the sync job should run\n    \"restartDependantDeployments\": true, // when a secret/cert changes from otoroshi sync, restart dependant deployments\n    \"templates\": { // template for entities that will be merged with kubernetes entities. can be \"default\" to use otoroshi default templates\n      \"service-group\": {},\n      \"service-descriptor\": {},\n      \"apikeys\": {},\n      \"global-config\": {},\n      \"jwt-verifier\": {},\n      \"tcp-service\": {},\n      \"certificate\": {},\n      \"auth-module\": {},\n      \"data-exporter\": {},\n      \"script\": {},\n      \"organization\": {},\n      \"team\": {},\n      \"data-exporter\": {}\n    }\n  }\n}\n```\n\nIf `endpoint` is not defined, Otoroshi will try to get it from `$KUBERNETES_SERVICE_HOST` and `$KUBERNETES_SERVICE_PORT`.\nIf `token` is not defined, Otoroshi will try to get it from the file at `/var/run/secrets/kubernetes.io/serviceaccount/token`.\nIf `caCert` is not defined, Otoroshi will try to get it from the file at `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt`.\nIf `$KUBECONFIG` is defined, `endpoint`, `token` and `caCert` will be read from the current context of the file referenced by it.\n\nyou can find a more complete example of the configuration object [here](https://github.com/MAIF/otoroshi/blob/master/otoroshi/app/plugins/jobs/kubernetes/config.scala#L134-L163)\n\n### Note about `apikeys` and `certificates` resources\n\nApikeys and Certificates are a little bit different than the other resources. They have ability to be defined without their secret part, but with an export setting so otoroshi will generate the secret parts and export the apikey or the certificate to kubernetes secret. Then any app will be able to mount them as volumes (see the full example below)\n\nIn those resources you can define \n\n```yaml\nexportSecret: true \nsecretName: the-secret-name\n```\n\nand omit `clientSecret` for apikey or `publicKey`, `privateKey` for certificates. For certificate you will have to provide a `csr` for the certificate in order to generate it\n\n```yaml\ncsr:\n  issuer: CN=Otoroshi Root\n  hosts: \n  - httpapp.foo.bar\n  - httpapps.foo.bar\n  key:\n    algo: rsa\n    size: 2048\n  subject: UID=httpapp-front, O=OtoroshiApps\n  client: false\n  ca: false\n  duration: 31536000000\n  signatureAlg: SHA256WithRSAEncryption\n  digestAlg: SHA-256\n```\n\nwhen apikeys are exported as kubernetes secrets, they will have the type `otoroshi.io/apikey-secret` with values `clientId` and `clientSecret`\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: apikey-1\ntype: otoroshi.io/apikey-secret\ndata:\n  clientId: TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyIGFkaXBpc2NpbmcgZWxpdA==\n  clientSecret: TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyIGFkaXBpc2NpbmcgZWxpdA==\n```\n\nwhen certificates are exported as kubernetes secrets, they will have the type `kubernetes.io/tls` with the standard values `tls.crt` (the full cert chain) and `tls.key` (the private key). For more convenience, they will also have a `cert.crt` value containing the actual certificate without the ca chain and `ca-chain.crt` containing the ca chain without the certificate.\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: certificate-1\ntype: kubernetes.io/tls\ndata:\n  tls.crt: TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyIGFkaXBpc2NpbmcgZWxpdA==\n  tls.key: TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyIGFkaXBpc2NpbmcgZWxpdA==\n  cert.crt: TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyIGFkaXBpc2NpbmcgZWxpdA==\n  ca-chain.crt: TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNlY3RldHVyIGFkaXBpc2NpbmcgZWxpdA== \n```\n\n## Full CRD example\n\nthen you can deploy the previous example with better configuration level, and using mtls, apikeys, etc\n\nLet say the app looks like :\n\n```js\nconst fs = require('fs'); \nconst https = require('https'); \n\n// here we read the apikey to access http-app-2 from files mounted from secrets\nconst clientId = fs.readFileSync('/var/run/secrets/kubernetes.io/apikeys/clientId').toString('utf8')\nconst clientSecret = fs.readFileSync('/var/run/secrets/kubernetes.io/apikeys/clientSecret').toString('utf8')\n\nconst backendKey = fs.readFileSync('/var/run/secrets/kubernetes.io/certs/backend/tls.key').toString('utf8')\nconst backendCert = fs.readFileSync('/var/run/secrets/kubernetes.io/certs/backend/cert.crt').toString('utf8')\nconst backendCa = fs.readFileSync('/var/run/secrets/kubernetes.io/certs/backend/ca-chain.crt').toString('utf8')\n\nconst clientKey = fs.readFileSync('/var/run/secrets/kubernetes.io/certs/client/tls.key').toString('utf8')\nconst clientCert = fs.readFileSync('/var/run/secrets/kubernetes.io/certs/client/cert.crt').toString('utf8')\nconst clientCa = fs.readFileSync('/var/run/secrets/kubernetes.io/certs/client/ca-chain.crt').toString('utf8')\n\nfunction callApi2() {\n  return new Promise((success, failure) => {\n    const options = { \n      // using the implicit internal name (*.global.otoroshi.mesh) of the other service descriptor passing through otoroshi\n      hostname: 'http-app-service-descriptor-2.global.otoroshi.mesh',  \n      port: 433, \n      path: '/', \n      method: 'GET',\n      headers: {\n        'Accept': 'application/json',\n        'Otoroshi-Client-Id': clientId,\n        'Otoroshi-Client-Secret': clientSecret,\n      },\n      cert: clientCert,\n      key: clientKey,\n      ca: clientCa\n    }; \n    let data = '';\n    const req = https.request(options, (res) => { \n      res.on('data', (d) => { \n        data = data + d.toString('utf8');\n      }); \n      res.on('end', () => { \n        success({ body: JSON.parse(data), res });\n      }); \n      res.on('error', (e) => { \n        failure(e);\n      }); \n    }); \n    req.end();\n  })\n}\n\nconst options = { \n  key: backendKey, \n  cert: backendCert, \n  ca: backendCa, \n  // we want mtls behavior\n  requestCert: true, \n  rejectUnauthorized: true\n}; \nhttps.createServer(options, (req, res) => { \n  res.writeHead(200, {'Content-Type': 'application/json'});\n  callApi2().then(resp => {\n    res.write(JSON.stringify{ (\"message\": `Hello to ${req.socket.getPeerCertificate().subject.CN}`, api2: resp.body })); \n  });\n}).listen(433);\n```\n\nthen, the descriptors will be :\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: http-app-deployment\nspec:\n  selector:\n    matchLabels:\n      run: http-app-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: http-app-deployment\n    spec:\n      containers:\n      - image: foo/http-app\n        imagePullPolicy: IfNotPresent\n        name: otoroshi\n        ports:\n          - containerPort: 443\n            name: \"https\"\n        volumeMounts:\n        - name: apikey-volume\n          # here you will be able to read apikey from files \n          # - /var/run/secrets/kubernetes.io/apikeys/clientId\n          # - /var/run/secrets/kubernetes.io/apikeys/clientSecret\n          mountPath: \"/var/run/secrets/kubernetes.io/apikeys\"\n          readOnly: true\n        volumeMounts:\n        - name: backend-cert-volume\n          # here you will be able to read app cert from files \n          # - /var/run/secrets/kubernetes.io/certs/backend/tls.crt\n          # - /var/run/secrets/kubernetes.io/certs/backend/tls.key\n          mountPath: \"/var/run/secrets/kubernetes.io/certs/backend\"\n          readOnly: true\n        - name: client-cert-volume\n          # here you will be able to read app cert from files \n          # - /var/run/secrets/kubernetes.io/certs/client/tls.crt\n          # - /var/run/secrets/kubernetes.io/certs/client/tls.key\n          mountPath: \"/var/run/secrets/kubernetes.io/certs/client\"\n          readOnly: true\n      volumes:\n      - name: apikey-volume\n        secret:\n          # here we reference the secret name from apikey http-app-2-apikey-1\n          secretName: secret-2\n      - name: backend-cert-volume\n        secret:\n          # here we reference the secret name from cert http-app-certificate-backend\n          secretName: http-app-certificate-backend-secret\n      - name: client-cert-volume\n        secret:\n          # here we reference the secret name from cert http-app-certificate-client\n          secretName: http-app-certificate-client-secret\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: http-app-service\nspec:\n  ports:\n    - port: 8443\n      targetPort: https\n      name: https\n  selector:\n    run: http-app-deployment\n---\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: ServiceGroup\nmetadata:\n  name: http-app-group\n  annotations:\n    otoroshi.io/id: http-app-group\nspec:\n  description: a group to hold services about the http-app\n---\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: ApiKey\nmetadata:\n  name: http-app-apikey-1\n# this apikey can be used to access the app\nspec:\n  # a secret name secret-1 will be created by otoroshi and can be used by containers\n  exportSecret: true \n  secretName: secret-1\n  authorizedEntities: \n  - group_http-app-group\n---\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: ApiKey\nmetadata:\n  name: http-app-2-apikey-1\n# this apikey can be used to access another app in a different group\nspec:\n  # a secret name secret-1 will be created by otoroshi and can be used by containers\n  exportSecret: true \n  secretName: secret-2\n  authorizedEntities: \n  - group_http-app-2-group\n---\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: http-app-certificate-frontend\nspec:\n  description: certificate for the http-app on otorshi frontend\n  autoRenew: true\n  csr:\n    issuer: CN=Otoroshi Root\n    hosts: \n    - httpapp.foo.bar\n    key:\n      algo: rsa\n      size: 2048\n    subject: UID=httpapp-front, O=OtoroshiApps\n    client: false\n    ca: false\n    duration: 31536000000\n    signatureAlg: SHA256WithRSAEncryption\n    digestAlg: SHA-256\n---\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: http-app-certificate-backend\nspec:\n  description: certificate for the http-app deployed on pods\n  autoRenew: true\n  # a secret name http-app-certificate-backend-secret will be created by otoroshi and can be used by containers\n  exportSecret: true \n  secretName: http-app-certificate-backend-secret\n  csr:\n    issuer: CN=Otoroshi Root\n    hosts: \n    - http-app-service \n    key:\n      algo: rsa\n      size: 2048\n    subject: UID=httpapp-back, O=OtoroshiApps\n    client: false\n    ca: false\n    duration: 31536000000\n    signatureAlg: SHA256WithRSAEncryption\n    digestAlg: SHA-256\n---\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: Certificate\nmetadata:\n  name: http-app-certificate-client\nspec:\n  description: certificate for the http-app\n  autoRenew: true\n  secretName: http-app-certificate-client-secret\n  csr:\n    issuer: CN=Otoroshi Root\n    key:\n      algo: rsa\n      size: 2048\n    subject: UID=httpapp-client, O=OtoroshiApps\n    client: false\n    ca: false\n    duration: 31536000000\n    signatureAlg: SHA256WithRSAEncryption\n    digestAlg: SHA-256\n---\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: ServiceDescriptor\nmetadata:\n  name: http-app-service-descriptor\nspec:\n  description: the service descriptor for the http app\n  groups: \n  - http-app-group\n  forceHttps: true\n  hosts:\n  - httpapp.foo.bar # hostname exposed oustide of the kubernetes cluster\n  # - http-app-service-descriptor.global.otoroshi.mesh # implicit internal name inside the kubernetes cluster \n  matchingRoot: /\n  targets:\n  - url: https://http-app-service:8443\n    # alternatively, you can use serviceName and servicePort to use pods ip addresses\n    # serviceName: http-app-service\n    # servicePort: https\n    mtlsConfig:\n      # use mtls to contact the backend\n      mtls: true\n      certs: \n        # reference the DN for the client cert\n        - UID=httpapp-client, O=OtoroshiApps\n      trustedCerts: \n        # reference the DN for the CA cert \n        - CN=Otoroshi Root\n  sendOtoroshiHeadersBack: true\n  xForwardedHeaders: true\n  overrideHost: true\n  allowHttp10: false\n  publicPatterns:\n    - /health\n  additionalHeaders:\n    x-foo: bar\n# here you can specify everything supported by otoroshi like jwt-verifiers, auth config, etc ... for more informations about it, just go to https://maif.github.io/otoroshi/swagger-ui/index.html\n```\n\nnow with this descriptor deployed, you can access your app with a command like \n\n```sh\nCLIENT_ID=`kubectl get secret secret-1 -o jsonpath=\"{.data.clientId}\" | base64 --decode`\nCLIENT_SECRET=`kubectl get secret secret-1 -o jsonpath=\"{.data.clientSecret}\" | base64 --decode`\ncurl -X GET https://httpapp.foo.bar/get -u \"$CLIENT_ID:$CLIENT_SECRET\"\n```\n\n## Expose Otoroshi to outside world\n\nIf you deploy Otoroshi on a kubernetes cluster, the Otoroshi service is deployed as a loadbalancer (service type: `LoadBalancer`). You'll need to declare in your DNS settings any name that can be routed by otoroshi going to the loadbalancer endpoint (CNAME or ip addresses) of your kubernetes distribution. If you use a managed kubernetes cluster from a cloud provider, it will work seamlessly as they will provide external loadbalancers out of the box. However, if you use a bare metal kubernetes cluster, id doesn't come with support for external loadbalancers (service of type `LoadBalancer`). So you will have to provide this feature in order to route external TCP traffic to Otoroshi containers running inside the kubernetes cluster. You can use projects like [MetalLB](https://metallb.universe.tf/) that provide software `LoadBalancer` services to bare metal clusters or you can use and customize examples in the installation section.\n\n@@@ warning\nWe don't recommand running Otoroshi behind an existing ingress controller (or something like that) as you will not be able to use features like TCP proxying, TLS, mTLS, etc. Also, this additional layer of reverse proxy will increase call latencies.\n@@@ \n\n## Access a service from inside the k8s cluster\n\n### Using host header overriding\n\nYou can access any service referenced in otoroshi, through otoroshi from inside the kubernetes cluster by using the otoroshi service name (if you use a template based on https://github.com/MAIF/otoroshi/tree/master/kubernetes/base deployed in the otoroshi namespace) and the host header with the service domain like :\n\n```sh\nCLIENT_ID=\"xxx\"\nCLIENT_SECRET=\"xxx\"\ncurl -X GET -H 'Host: httpapp.foo.bar' https://otoroshi-service.otoroshi.svc.cluster.local:8443/get -u \"$CLIENT_ID:$CLIENT_SECRET\"\n```\n\n### Using dedicated services\n\nit's also possible to define services that targets otoroshi deployment (or otoroshi workers deployment) and use then as valid hosts in otoroshi services \n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-awesome-service\nspec:\n  selector:\n    # run: otoroshi-deployment\n    # or in cluster mode\n    run: otoroshi-worker-deployment\n  ports:\n  - port: 8080\n    name: \"http\"\n    targetPort: \"http\"\n  - port: 8443\n    name: \"https\"\n    targetPort: \"https\"\n```\n\nand access it like\n\n```sh\nCLIENT_ID=\"xxx\"\nCLIENT_SECRET=\"xxx\"\ncurl -X GET https://my-awesome-service.my-namspace.svc.cluster.local:8443/get -u \"$CLIENT_ID:$CLIENT_SECRET\"\n```\n\n### Using coredns integration\n\nYou can also enable the coredns integration to simplify the flow. You can use the the following keys in the plugin config :\n\n```javascript\n{\n  \"KubernetesConfig\": {\n    ...\n    \"coreDnsIntegration\": true,                // enable coredns integration for intra cluster calls\n    \"kubeSystemNamespace\": \"kube-system\",      // the namespace where coredns is deployed\n    \"corednsConfigMap\": \"coredns\",             // the name of the coredns configmap\n    \"otoroshiServiceName\": \"otoroshi-service\", // the name of the otoroshi service, could be otoroshi-workers-service\n    \"otoroshiNamespace\": \"otoroshi\",           // the namespace where otoroshi is deployed\n    \"clusterDomain\": \"cluster.local\",          // the domain for cluster services\n    ...\n  }\n}\n```\n\notoroshi will patch coredns config at startup then you can call your services like\n\n```sh\nCLIENT_ID=\"xxx\"\nCLIENT_SECRET=\"xxx\"\ncurl -X GET https://my-awesome-service.my-awesome-service-namespace.otoroshi.mesh:8443/get -u \"$CLIENT_ID:$CLIENT_SECRET\"\n```\n\nBy default, all services created from CRDs service descriptors are exposed as `${service-name}.${service-namespace}.otoroshi.mesh` or `${service-name}.${service-namespace}.svc.otoroshi.local`\n\n### Using coredns with manual patching\n\nyou can also patch the coredns config manually\n\n```sh\nkubectl edit configmaps coredns -n kube-system # or your own custom config map\n```\n\nand change the `Corefile` data to add the following snippet in at the end of the file\n\n```yaml\notoroshi.mesh:53 {\n    errors\n    health\n    ready\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n        pods insecure\n        upstream\n        fallthrough in-addr.arpa ip6.arpa\n    }\n    rewrite name regex (.*)\\.otoroshi\\.mesh otoroshi-worker-service.otoroshi.svc.cluster.local\n    forward . /etc/resolv.conf\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n```\n\nyou can also define simpler rewrite if it suits you use case better\n\n```\nrewrite name my-service.otoroshi.mesh otoroshi-worker-service.otoroshi.svc.cluster.local\n```\n\ndo not hesitate to change `otoroshi-worker-service.otoroshi` according to your own setup. If otoroshi is not in cluster mode, change it to `otoroshi-service.otoroshi`. If otoroshi is not deployed in the `otoroshi` namespace, change it to `otoroshi-service.the-namespace`, etc.\n\nBy default, all services created from CRDs service descriptors are exposed as `${service-name}.${service-namespace}.otoroshi.mesh`\n\nthen you can call your service like \n\n```sh\nCLIENT_ID=\"xxx\"\nCLIENT_SECRET=\"xxx\"\n\ncurl -X GET https://my-awesome-service.my-awesome-service-namespace.otoroshi.mesh:8443/get -u \"$CLIENT_ID:$CLIENT_SECRET\"\n```\n\n### Using old kube-dns system\n\nif your stuck with an old version of kubernetes, it uses kube-dns that is not supported by otoroshi, so you will have to provide your own coredns deployment and declare it as a stubDomain in the old kube-dns system. \n\nHere is an example of coredns deployment with otoroshi domain config\n\ncoredns.yaml\n:   @@snip [coredns.yaml](../snippets/kubernetes/kustomize/base/coredns.yaml)\n\nthen you can enable the kube-dns integration in the otoroshi kubernetes job\n\n```javascript\n{\n  \"KubernetesConfig\": {\n    ...\n    \"kubeDnsOperatorIntegration\": true,                // enable kube-dns integration for intra cluster calls\n    \"kubeDnsOperatorCoreDnsNamespace\": \"otoroshi\",    // namespace where coredns is installed\n    \"kubeDnsOperatorCoreDnsName\": \"otoroshi-dns\",     // name of the coredns service\n    \"kubeDnsOperatorCoreDnsPort\": 5353,               // port of the coredns service\n    ...\n  }\n}\n```\n\n### Using Openshift DNS operator\n\nOpenshift DNS operator does not allow to customize DNS configuration a lot, so you will have to provide your own coredns deployment and declare it as a stub in the Openshift DNS operator. \n\nHere is an example of coredns deployment with otoroshi domain config\n\ncoredns.yaml\n:   @@snip [coredns.yaml](../snippets/kubernetes/kustomize/base/coredns.yaml)\n\nthen you can enable the Openshift DNS operator integration in the otoroshi kubernetes job\n\n```javascript\n{\n  \"KubernetesConfig\": {\n    ...\n    \"openshiftDnsOperatorIntegration\": true,                // enable openshift dns operator integration for intra cluster calls\n    \"openshiftDnsOperatorCoreDnsNamespace\": \"otoroshi\",    // namespace where coredns is installed\n    \"openshiftDnsOperatorCoreDnsName\": \"otoroshi-dns\",     // name of the coredns service\n    \"openshiftDnsOperatorCoreDnsPort\": 5353,               // port of the coredns service\n    ...\n  }\n}\n```\n\ndon't forget to update the otoroshi `ClusterRole`\n\n```yaml\n- apiGroups:\n    - operator.openshift.io\n  resources:\n    - dnses\n  verbs:\n    - get\n    - list\n    - watch\n    - update\n```\n\n## CRD validation in kubectl\n\nIn order to get CRD validation before manifest deployments right inside kubectl, you can deploy a validation webhook that will do the trick. Also check that you have `otoroshi.plugins.jobs.kubernetes.KubernetesAdmissionWebhookCRDValidator` request sink enabled.\n\nvalidation-webhook.yaml\n:   @@snip [validation-webhook.yaml](../snippets/kubernetes/kustomize/base/validation-webhook.yaml)\n\n## Easier integration with otoroshi-sidecar\n\nOtoroshi can help you to easily use existing services without modifications while gettings all the perks of otoroshi like apikeys, mTLS, exchange protocol, etc. To do so, otoroshi will inject a sidecar container in the pod of your deployment that will handle call coming from otoroshi and going to otoroshi. To enable otoroshi-sidecar, you need to deploy the following admission webhook. Also check that you have `otoroshi.plugins.jobs.kubernetes.KubernetesAdmissionWebhookSidecarInjector` request sink enabled.\n\nsidecar-webhook.yaml\n:   @@snip [sidecar-webhook.yaml](../snippets/kubernetes/kustomize/base/sidecar-webhook.yaml)\n\nthen it's quite easy to add the sidecar, just add the following label to your pod `otoroshi.io/sidecar: inject` and some annotations to tell otoroshi what certificates and apikeys to use.\n\n```yaml\nannotations:\n  otoroshi.io/sidecar-apikey: backend-apikey\n  otoroshi.io/sidecar-backend-cert: backend-cert\n  otoroshi.io/sidecar-client-cert: oto-client-cert\n  otoroshi.io/token-secret: secret\n  otoroshi.io/expected-dn: UID=oto-client-cert, O=OtoroshiApps\n```\n\nnow you can just call you otoroshi handled apis from inside your pod like `curl http://my-service.namespace.otoroshi.mesh/api` without passing any apikey or client certificate and the sidecar will handle everything for you. Same thing for call from otoroshi to your pod, everything will be done in mTLS fashion with apikeys and otoroshi exchange protocol\n\nhere is a full example\n\nsidecar.yaml\n:   @@snip [sidecar.yaml](../snippets/kubernetes/kustomize/base/sidecar.yaml)\n\n@@@ warning\nPlease avoid to use port `80` for your pod as it's the default port to access otoroshi from your pod and the call will be redirect to the sidecar via an iptables rule\n@@@\n\n## Daikoku integration\n\nIt is possible to easily integrate daikoku generated apikeys without any human interaction with the actual apikey secret. To do that, create a plan in Daikoku and setup the integration mode to `Automatic`\n\n@@@ div { .centered-img }\n<img src=\"../imgs/kubernetes-daikoku-integration-enabled.png\" />\n@@@\n\nthen when a user subscribe for an apikey, he will only see an integration token\n\n@@@ div { .centered-img }\n<img src=\"../imgs/kubernetes-daikoku-integration-token.png\" />\n@@@\n\nthen just create an ApiKey manifest with this token and your good to go \n\n```yaml\napiVersion: proxy.otoroshi.io/v1alpha1\nkind: ApiKey\nmetadata:\n  name: http-app-2-apikey-3\nspec:\n  exportSecret: true \n  secretName: secret-3\n  daikokuToken: RShQrvINByiuieiaCBwIZfGFgdPu7tIJEN5gdV8N8YeH4RI9ErPYJzkuFyAkZ2xy\n```\n\n"
  },
  {
    "name": "other.md",
    "id": "/deploy/other.md",
    "url": "/deploy/other.html",
    "title": "Others",
    "content": "# Others\n\nOtoroshi can run wherever you want, even on a raspberry pi (Cluster^^) ;)\n\nThis section is not finished yet. So, as Otoroshi is available as a @ref:[Docker image](../install/get-otoroshi.md#from-docker) that you can run on any Docker compatible cloud, just go ahead and use it on cloud provider until we have more detailed documentation.\n\n## Running Otoroshi on AWS Elastic Beanstalk\n\nSee the @ref:[dedicated page to run Otoroshi on AWS Elastic Beanstalk](./aws.md)\n\n## Running Otoroshi on Amazon Elastic Container Service\n\nDeploy the @ref:[Docker image](../install/get-otoroshi.md#from-docker) using [Amazon ECS](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html)\n\n## Running Otoroshi on GCE\n\nDeploy the @ref:[Docker image](../install/get-otoroshi.md#from-docker) using [Google Compute Engine container integration](https://cloud.google.com/compute/docs/containers/deploying-containers)\n\n## Running Otoroshi on Azure\n\nDeploy the @ref:[Docker image](../install/get-otoroshi.md#from-docker) using [Azure Container Service](https://azure.microsoft.com/en-us/services/container-service/)\n\n## Running Otoroshi on Heroku\n\nDeploy the @ref:[Docker image](../install/get-otoroshi.md#from-docker) using [Docker integration](https://devcenter.heroku.com/articles/container-registry-and-runtime)\n\n## Running Otoroshi on CloudFoundry\n\nDeploy the @ref:[Docker image](../install/get-otoroshi.md#from-docker) using [Docker integration](https://docs.cloudfoundry.org/adminguide/docker.html)\n\n## Running Otoroshi on your own infrastructure\n\nAs Otoroshi is a [Play Framework](https://www.playframework.com) application, you can read the doc about putting a `Play` app in production.\n\nhttps://www.playframework.com/documentation/2.6.x/ProductionConfiguration\n\nDownload the latest @ref:[Otoroshi distribution](../install/get-otoroshi.md#from-zip), unzip it, customize it and run it.\n"
  },
  {
    "name": "scaling.md",
    "id": "/deploy/scaling.md",
    "url": "/deploy/scaling.html",
    "title": "Scaling Otoroshi",
    "content": "# Scaling Otoroshi\n\n## Using multiple instances with a front load balancer\n\nOtoroshi has been designed to work with multiple instances. If you already have an infrastructure using frontal load balancing, you just have to declare Otoroshi instances as the target of all domain names handled by Otoroshi\n\n## Using master / workers mode of Otoroshi\n\nYou can read everything about it in @ref:[the clustering section](../deploy/clustering.md) of the documentation.\n\n## Using IPVS\n\nYou can use [IPVS](https://en.wikipedia.org/wiki/IP_Virtual_Server) to load balance layer 4 traffic directly from the Linux Kernel to multiple instances of Otoroshi. You can find example of configuration [here](http://www.linuxvirtualserver.org/VS-DRouting.html) \n\n## Using DNS Round Robin\n\nYou can use [DNS round robin technique](https://en.wikipedia.org/wiki/Round-robin_DNS) to declare multiple A records under the domain names handled by Otoroshi.\n\n## Using software L4/L7 load balancers\n\nYou can use software L4 load balancers like NGINX or HAProxy to load balance layer 4 traffic directly from the Linux Kernel to multiple instances of Otoroshi.\n\nNGINX L7\n:   @@snip [nginx-http.conf](../snippets/nginx-http.conf) \n\nNGINX L4\n:   @@snip [nginx-tcp.conf](../snippets/nginx-tcp.conf) \n\nHA Proxy L7\n:   @@snip [haproxy-http.conf](../snippets/haproxy-http.conf) \n\nHA Proxy L4\n:   @@snip [haproxy-tcp.conf](../snippets/haproxy-tcp.conf) \n\n## Using a custom TCP load balancer\n\nYou can also use any other TCP load balancer, from a hardware box to a small js file like\n\ntcp-proxy.js\n:   @@snip [tcp-proxy.js](../snippets/tcp-proxy.js) \n\ntcp-proxy.rs\n:   @@snip [tcp-proxy.rs](../snippets/proxy.rs) \n\n"
  },
  {
    "name": "dev.md",
    "id": "/dev.md",
    "url": "/dev.html",
    "title": "Developing Otoroshi",
    "content": "# Developing Otoroshi\n\nIf you want to play with Otoroshis code, here are some tips\n\n## The tools\n\nYou will need\n\n* git\n* JDK 11\n* SBT 1.3.x\n* Node 13 + yarn 1.x\n\n## Clone the repository\n\n```sh\ngit clone https://github.com/MAIF/otoroshi.git\n```\n\nor fork otoroshi and clone your own repository.\n\n## Run otoroshi in dev mode\n\nto run otoroshi in dev mode, you'll need to run two separate process to serve the javascript UI and the server part.\n\n### Javascript side\n\njust go to `<repo>/otoroshi/javascript` and install the dependencies with\n\n```sh\nyarn install\n# or\nnpm install\n```\n\nthen run the dev server with\n\n```sh\nyarn start\n# or\nnpm run start\n```\n\n### Server side\n\nsetup SBT opts with\n\n```sh\nexport SBT_OPTS=\"-Xmx2G -Xss6M\"\n```\n\nthen just go to `<repo>/otoroshi` and run the sbt console with \n\n```sh\nsbt\n```\n\nthen in the sbt console run the following command\n\n```sh\n~run -Dapp.storage=file -Dapp.liveJs=true -Dhttps.port=9998 -D-Dapp.privateapps.port=9999 -Dapp.adminPassword=password -Dapp.domain=oto.tools -Dplay.server.https.engineProvider=ssl.DynamicSSLEngineProvider -Dapp.events.maxSize=0\n```\n\nyou can now access your otoroshi instance at `http://otoroshi.oto.tools:9999`\n\n## Test otoroshi\n\nto run otoroshi test just go to `<repo>/otoroshi` and run the main test suite with\n\n```sh\nsbt 'testOnly OtoroshiTests'\n```\n\n## Create a release\n\njust go to `<repo>/otoroshi/javascript` and then build the UI\n\n```sh\nyarn install\nyarn build\n```\n\nthen go to `<repo>/otoroshi` and build the otoroshi distribution\n\n```sh\nsbt ';clean;compile;dist;assembly'\n```\n\nthe otoroshi build is waiting for you in `<repo>/otoroshi/target/scala-2.12/otoroshi.jar` or `<repo>/otoroshi/target/universal/otoroshi-1.x.x.zip`\n\n## Build the documentation\n\nfrom the root of your repository run\n\n```sh\nsh ./scripts/doc.sh all\n```\n\n## Format the sources\n\nfrom the root of your repository run\n\n```sh\nsh ./scripts/fmt.sh\n```"
  },
  {
    "name": "apikeys.md",
    "id": "/entities/apikeys.md",
    "url": "/entities/apikeys.html",
    "title": "Apikeys",
    "content": "# Apikeys\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "auth-modules.md",
    "id": "/entities/auth-modules.md",
    "url": "/entities/auth-modules.html",
    "title": "Authentication modules",
    "content": "# Authentication modules\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "certificates.md",
    "id": "/entities/certificates.md",
    "url": "/entities/certificates.html",
    "title": "Certificates",
    "content": "# Certificates\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "data-exporters.md",
    "id": "/entities/data-exporters.md",
    "url": "/entities/data-exporters.html",
    "title": "Data exporters",
    "content": "# Data exporters\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "global-config.md",
    "id": "/entities/global-config.md",
    "url": "/entities/global-config.html",
    "title": "Global config",
    "content": "# Global config\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "index.md",
    "id": "/entities/index.md",
    "url": "/entities/index.html",
    "title": "",
    "content": "\n# Main entities\n\nin this section, we will pass through all the main Otoroshi entities\n\n* @ref:[Organizations](./organizations.md)\n* @ref:[Teams](./teams.md)\n* @ref:[Global Config](./global-config.md)\n* @ref:[Apikeys](./apikeys.md)\n* @ref:[Service groups](./service-groups.md)\n* @ref:[Service descriptors](./service-descriptors.md)\n* @ref:[Auth. modules](./auth-modules.md)\n* @ref:[Certificates](./certificates.md)\n* @ref:[JWT verifiers](./jwt-verifiers.md)\n* @ref:[Data exporters](./data-exporters.md)\n* @ref:[Scripts](./scripts.md)\n* @ref:[TCP services](./tcp-services.md)\n\n@@@ index\n\n* [Organizations](./organizations.md)\n* [Teams](./teams.md)\n* [Global Config](./global-config.md)\n* [Apikeys](./apikeys.md)\n* [Service groups](./service-groups.md)\n* [Service descriptors](./service-descriptors.md)\n* [Auth. modules](./auth-modules.md)\n* [Certificates](./certificates.md)\n* [JWT verifiers](./jwt-verifiers.md)\n* [Data exporters](./data-exporters.md)\n* [Scripts](./scripts.md)\n* [TCP services](./tcp-services.md)\n\n@@@\n"
  },
  {
    "name": "jwt-verifiers.md",
    "id": "/entities/jwt-verifiers.md",
    "url": "/entities/jwt-verifiers.html",
    "title": "JWT verifiers",
    "content": "# JWT verifiers\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "organizations.md",
    "id": "/entities/organizations.md",
    "url": "/entities/organizations.html",
    "title": "Organizations",
    "content": "# Organizations\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "scripts.md",
    "id": "/entities/scripts.md",
    "url": "/entities/scripts.html",
    "title": "Scripts",
    "content": "# Scripts\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "service-descriptors.md",
    "id": "/entities/service-descriptors.md",
    "url": "/entities/service-descriptors.html",
    "title": "Service descriptors",
    "content": "# Service descriptors\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "service-groups.md",
    "id": "/entities/service-groups.md",
    "url": "/entities/service-groups.html",
    "title": "Service groups",
    "content": "# Service groups\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "tcp-services.md",
    "id": "/entities/tcp-services.md",
    "url": "/entities/tcp-services.html",
    "title": "TCP services",
    "content": "# TCP services\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "teams.md",
    "id": "/entities/teams.md",
    "url": "/entities/teams.html",
    "title": "Teams",
    "content": "# Teams\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "features.md",
    "id": "/features.md",
    "url": "/features.html",
    "title": "Features",
    "content": "# Features\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "getting-started.md",
    "id": "/getting-started.md",
    "url": "/getting-started.html",
    "title": "Getting Started",
    "content": "# Getting Started\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "end-to-end-mtls.md",
    "id": "/how-to-s/end-to-end-mtls.md",
    "url": "/how-to-s/end-to-end-mtls.html",
    "title": "End-to-end mTLS",
    "content": "# End-to-end mTLS\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "export-alerts-using-mailgun.md",
    "id": "/how-to-s/export-alerts-using-mailgun.md",
    "url": "/how-to-s/export-alerts-using-mailgun.html",
    "title": "Send alerts using mailgun",
    "content": "# Send alerts using mailgun\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "export-events-to-elastic.md",
    "id": "/how-to-s/export-events-to-elastic.md",
    "url": "/how-to-s/export-events-to-elastic.html",
    "title": "Export events to Elasticsearch",
    "content": "# Export events to Elasticsearch\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "import-export-otoroshi-datastore.md",
    "id": "/how-to-s/import-export-otoroshi-datastore.md",
    "url": "/how-to-s/import-export-otoroshi-datastore.html",
    "title": "Import / export otoroshi datastore",
    "content": "# Import / export otoroshi datastore\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "index.md",
    "id": "/how-to-s/index.md",
    "url": "/how-to-s/index.html",
    "title": "",
    "content": "\n# How to's\n\nin this section, we will explain some mainstream Otoroshi usage scenario's \n\n* @ref:[End-to-end mTLS](./end-to-end-mtls.md)\n* @ref:[Send alerts by emails](./export-alerts-using-mailgun.md)\n* @ref:[Export events to Elasticsearch](./export-events-to-elastic.md)\n* @ref:[Import/export Otoroshi datastore](./import-export-otoroshi-datastore.md)\n* @ref:[Secure an app with Auth0](./secure-app-with-auth0.md)\n* @ref:[Secure an app with Keycloak](./secure-app-with-keycloak.md)\n* @ref:[Secure an app with LDAP](./secure-app-with-ldap.md)\n* @ref:[Secure an api with apikeys](./secure-with-apikey.md)\n* @ref:[Secure an api with OAuth2 client_credentials flow](./secure-with-oauth2-client-credentials.md)\n* @ref:[Setup an Otoroshi cluster](./setup-otoroshi-cluster.md)\n* @ref:[TLS termination using Let's Encrypt](./tls-using-lets-encrypt.md)\n\n@@@ index\n\n* [End-to-end mTLS](./end-to-end-mtls.md)\n* [Send alerts by emails](./export-alerts-using-mailgun.md)\n* [Export events to Elasticsearch](./export-events-to-elastic.md)\n* [Import/export Otoroshi datastore](./import-export-otoroshi-datastore.md)\n* [Secure an app with Auth0](./secure-app-with-auth0.md)\n* [Secure an app with Keycloak](./secure-app-with-keycloak.md)\n* [Secure an app with LDAP](./secure-app-with-ldap.md)\n* [Secure an api with apikeys](./secure-with-apikey.md)\n* [Secure an api with OAuth2 client_credentials flow](./secure-with-oauth2-client-credentials.md)\n* [Setup an Otoroshi cluster](./setup-otoroshi-cluster.md)\n* [TLS termination using Let's Encrypt](./tls-using-lets-encrypt.md)\n\n@@@"
  },
  {
    "name": "secure-app-with-auth0.md",
    "id": "/how-to-s/secure-app-with-auth0.md",
    "url": "/how-to-s/secure-app-with-auth0.html",
    "title": "Secure an app with Auth0",
    "content": "# Secure an app with Auth0\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "secure-app-with-keycloak.md",
    "id": "/how-to-s/secure-app-with-keycloak.md",
    "url": "/how-to-s/secure-app-with-keycloak.html",
    "title": "Secure an app with Keycloak",
    "content": "# Secure an app with Keycloak\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "secure-app-with-ldap.md",
    "id": "/how-to-s/secure-app-with-ldap.md",
    "url": "/how-to-s/secure-app-with-ldap.html",
    "title": "Secure an app and/or your Otoroshi UI with LDAP",
    "content": "# Secure an app and/or your Otoroshi UI with LDAP\n\n### Cover by this tutorial\n\n- [Secure an app and/or your Otoroshi UI with LDAP](#secure-an-app-andor-your-otoroshi-ui-with-ldap)\n- [Cover by this tutorial](#cover-by-this-tutorial)\n- [Download Otoroshi](#download-otoroshi)\n- [Create an Authentication configuration](#create-an-authentication-configuration)\n- [Connect to Otoroshi with LDAP authentication](#connect-to-otoroshi-with-ldap-authentication)\n- [Testing your configuration](#testing-your-configuration)\n- [Secure an app with LDAP authentication](#secure-an-app-with-ldap-authentication)\n- [Manage LDAP users rights on Otoroshi](#manage-ldap-users-rights-on-otoroshi)\n- [Advanced usage of LDAP Authentication](#advanced-usage-of-ldap-authentication)\n\n<img src=\"../imgs/ldap-tutorial.png\" height=\"400px\"> \n\n### Download Otoroshi\n\nLet's start by downloading the latest Otoroshi\n```sh\ncurl -L -o otoroshi.jar 'https://github.com/MAIF/otoroshi/releases/download/v1.5.0-dev/otoroshi.jar'\n```\n\nBy default, Otoroshi starts with domain `oto.tools` that targets `127.0.0.1`\n```sh\nsudo nano /etc/hosts\n\n# Add this line at the bottom of your file\n127.0.0.1\totoroshi.oto.tools privateapps.oto.tools otoroshi-api.oto.tools otoroshi-admin-internal-api.oto.tools localhost\n```\n\nRun Otoroshi\n```sh\njava -jar otoroshi.jar\n```\n\nThis should display\n\n```sh\n$ java -jar otoroshi.jar\n\n[info] otoroshi-env - Otoroshi version 1.5.0-beta.7\n[info] otoroshi-env - Admin API exposed on http://otoroshi-api.oto.tools:9999\n[info] otoroshi-env - Admin UI  exposed on http://otoroshi.oto.tools:9999\n...\n[info] p.c.s.AkkaHttpServer - Listening for HTTP on /0:0:0:0:0:0:0:0:9999\n[info] p.c.s.AkkaHttpServer - Listening for HTTPS on /0:0:0:0:0:0:0:0:9998\n[info] otoroshi-script-manager - Finding and starting plugins done in 4681 ms.\n...\n```\n\n#### Running an simple OpenLDAP server \n\nRun OpenLDAP docker image : \n```sh\ndocker run \\\n-p 389:389 \\\n-p 636:636  \\\n--env LDAP_ORGANISATION=\"Otoroshi company\" \\\n--env LDAP_DOMAIN=\"otoroshi.tools\" \\\n--env LDAP_ADMIN_PASSWORD=\"otoroshi\" \\\n--env LDAP_READONLY_USER=\"false\" \\\n--env LDAP_TLS\"false\" \\\n--env LDAP_TLS_ENFORCE\"false\" \\\n--name my-openldap-container \\\n--detach osixia/openldap:1.5.0\n```\n\nLet's make the first search in our LDAP container :\n```sh\ndocker exec my-openldap-container ldapsearch -x -H ldap://localhost -b dc=otoroshi,dc=tools -D \"cn=admin,dc=otoroshi,dc=tools\" -w otoroshi\n```\n\nThis should output :\n```sh\n# extended LDIF\n ...\n# otoroshi.tools\ndn: dc=otoroshi,dc=tools\nobjectClass: top\nobjectClass: dcObject\nobjectClass: organization\no: Otoroshi company\ndc: otoroshi\n\n# search result\nsearch: 2\nresult: 0 Success\n...\n```\n\nNow you can seed the open LDAP server with a few users. \n\nJoin your LDAP container.\n```sh\ndocker exec -it my-openldap-container \"/bin/bash\"\n```\n\nThe command `ldapadd` needs of a file to run.\n\nLaunch this command to create a `bootstrap.ldif` with one organization, one singers group with Johnny user and a last group with Einstein as scientist.\n```sh\necho -e \"\ndn: ou=People,dc=otoroshi,dc=tools\nobjectclass: top\nobjectclass: organizationalUnit\nou: People\n\ndn: ou=Role,dc=otoroshi,dc=tools\nobjectclass: top\nobjectclass: organizationalUnit\nou: Role\n\ndn: uid=johnny,ou=People,dc=otoroshi,dc=tools\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nuid: johnny\ncn: Jhonny\nsn: Brown\nmail: johnny@otoroshi.tools\npostalCode: 88442\nuserPassword: password\n\ndn: uid=einstein,ou=People,dc=otoroshi,dc=tools\nobjectclass: top\nobjectclass: person\nobjectclass: organizationalPerson\nobjectclass: inetOrgPerson\nuid: einstein\ncn: Einstein\nsn: Wilson\nmail: einstein@otoroshi.tools\npostalCode: 88443\nuserPassword: password\n\ndn: cn=singers,ou=Role,dc=otoroshi,dc=tools\nobjectclass: top\nobjectclass: groupOfNames\ncn: singers\nmember: uid=johnny,ou=People,dc=otoroshi,dc=tools\n\ndn: cn=scientists,ou=Role,dc=otoroshi,dc=tools\nobjectclass: top\nobjectclass: groupOfNames\ncn: scientists\nmember: uid=einstein,ou=People,dc=otoroshi,dc=tools\n\" > bootstrap.ldif\n\nldapadd -x -w otoroshi -D \"cn=admin,dc=otoroshi,dc=tools\" -f bootstrap.ldif -v\n```\n\n### Create an Authentication configuration\n\n1. Go ahead, and navigate to http://otoroshi.oto.tools:9999\n1. Click on the cog icon on the top right\n1. Then `Authentication confis` button\n1. And add a new configuration when clicking on the `Add item` button\n1. Select the `Ldap auth. provider` in the type selector field\n1. Set a basic name and description\n1. Then set `ldap://localhost:389` as `LDAP Server URL`and `dc=otoroshi,dc=tools` as `Search Base`\n1. Create a group filter (in the next part, we'll change this filter to spread users in different groups with given rights) with \n\nobjectClass=groupOfNames as *Group filter* \\\nAll as *Tenant*\\\nAll as *Team*\\\nRead/Write as *Rights*\n\n9. Set the search filter as (uid=${username})`\n1. Set `cn=admin,dc=otoroshi,dc=tools` as *Admin username*\n1. Set `otoroshi` as *Admin password*\n2. At the bottom of the page, disable the `secure` button (because we're using http and this configuration avoid to include cookie in an HTTP Request without secure channel, typically HTTPs)\n\n\n At this point, your configuration should be similar to :\n<!-- oto-scenario\n - goto /bo/dashboard/auth-configs/edit/auth_mod_09975547-a186-4a2d-a550-ca71a0a03c0c\n - wait 1000\n - screenshot-area generated-hows-to-ldap-auth-configs.png #app>div>div.container-fluid>div>div.col-sm-10.col-sm-offset-2.main\n-->\n<img src=\"../imgs/generated-hows-to-ldap-auth-configs.png\" />\n\n> Dont' forget to save on the bottom page your configuration before to quit the page.\n\n12. Test the connection when clicking on `Test admin connection` button\n\nThis should display a `It works!` message\n\n13. Finally, test the user connection button and set `johnny/password` or `einstein/password` as credentials.\n\nThis should display a `It works!` message\n\n> Dont' forget to save on the bottom page your configuration before to quit the page.\n\n### Connect to Otoroshi with LDAP authentication\n\nTo secure Otoroshi with your LDAP configuration, we have to register an Authentication configuration as a BackOffice Auth. configuration.\n\n1. Navigate to the *danger zone* (when clicking on the cog on the top right and selecting Danger zone)\n1. Scroll to the *BackOffice auth. settings*\n1. Select your last Authentication configuration (created in the previous section)\n1. Save the global configuration with the button on the top right\n\n#### Testing your configuration\n\n1. Disconnect from your instance\n1. Then click on the *Login using third-party* button (or navigate to *http://otoroshi.oto.tools:9999/backoffice/auth0/login*)\n1. Set `johnny/password` or `einstein/password` as credentials\n\n> A fallback solution is always available, by going to *http://otoroshi.oto.tools:9999/bo/simple/login*, for administrators in case your LDAP is not available\n\n\n#### Secure an app with LDAP authentication\n\nOnce the configuration is done, you can secure any of Otoroshi services with it. \n\n1. Navigate to any created service\n2. Jump to the `URL Patterns` section\n3. Enable your service as `Public UI`\n4. Then scroll to `Authentication` section\n5. Enable `Enforce user authentication`\n6. Select your Authentication config inside the list\n7. Enable `Strict mode`\n\n<!-- oto-scenario\n - goto /bo/dashboard/lines/prod/services/service_mirror_opunmaif_fr\n - wait 1000\n - click div[data-screenshot=\"ldap-tutorial-authentication\"]>div:nth-child(2)>div\n - screenshot-area generated-hows-to-secure-an-app-with-ldap.png div[data-screenshot=\"ldap-tutorial-authentication\"]\n-->\n<img src=\"../imgs/generated-hows-to-secure-an-app-with-ldap.png\">\n\n#### Manage LDAP users rights on Otoroshi\n\nFor each LDAP groups, you can affect a list of rights : \n- on an `Organization` : only ressources of an organization\n- on a `Team` : only ressources belonging to this team\n- and a level of rights : `Read`, `Write` or `Read/Write`\n\n\nStart by navigate to your authentication configuration (created in [previous](#create-an-authentication-configuration) step).\n\nThen, replace the values of the `Mapping group filter` field to match LDAP groups with Otoroshi rights.\n\n<!-- oto-scenario\n - goto /bo/dashboard/auth-configs/edit/auth_mod_91bb8b38-620e-4c18-9bbc-7c8d1efd63cc\n - wait 1000\n - screenshot-area generated-hows-to-ldap-manage-users.png #app>div>div.container-fluid>div>div.col-sm-10.col-sm-offset-2.main>div>div> div.row>div>div>div>form>div>div:nth-child(3)>div:nth-child(11)\n-->\n<img src=\"../imgs/generated-hows-to-ldap-manage-users.png\" />\n\nWith this configuration, Einstein is an administrator of Otoroshi with full rights (read / write) on all organizations.\n\nConversely, Johnny can't see any configuration pages (like the danger zone) because he has only the read rights on Otoroshi.\n\nYou can easily test this behaviour by [testing](#testing-your-configuration) with both credentials.\n\n\n#### Advanced usage of LDAP Authentication\n\nIn the previous section, we have set rights for each LDAP groups. But in some case, we want to have a finer granularity like set rights for a specific user.\n\nThe last 4 fields of the authentication form cover this. \n\nEach field take the `Email field name` as keys, and a json value.\n\nLet's start by adding few properties for each connected users with `Extra metadata`.\n\n```json\n// Add this configuration in extra metadata part\n{\n  \"provider\": \"OpenLDAP\"\n}\n```\n\nThe next field `Data override` is merged with extra metadata when a user connects to a `private app` (Inside Otoroshi, private app is a service secure by any authentication plugin) or to the UI. The `Email field name` is configured to match with the `mail` field from LDAP user data.\n\n```json \n{\n  \"johnny@otoroshi.tools\": {\n    \"stage_name\": \"Jean-Philippe Smet\"\n  }\n}\n```\n\nIf you try to connect to an app with this configuration, the user result profile should be :\n```json\n{\n  ...,\n  \"metadata\": {\n    \"lastname\": \"Hallyday\",\n    \"stage_name\": \"Jean-Philippe Smet\"\n  }\n}\n```\n\nLet's try to increase the Johnny rights with the `Additional rights group`.\n\nThis field supports the creation of virtual groups. A virtual group is composed of a list of users and a list of rights for each teams/organizations.\n\n```json\n// increase_johnny_rights is a virtual group which adds full access rights at johnny \n{\n  \"increase_johnny_rights\": {\n    \"rights\": [\n      {\n        \"tenant\": \"*:rw\",\n        \"teams\": [\n          \"*:rw\"\n        ]\n      }\n    ],\n    \"users\": [\n      \"jhonny@otoroshi.tools\"\n    ]\n  }\n}\n```\n\nThe last field `Rights override` is useful when you want erase the rights of an user with only specific rights.\n\nThe rights override field is the last to be applied on the user rights. \n\nTo resume, when Johnny connects to Otoroshi, he receives the rights to read only on the default Organization (from `Mapping group filter`), then he is promote to administrator role (from `Additional rights group`) and finally his rights are reset with the last field `Rights override` to the read rights.\n\n```json \n{\n  \"jhonny@otoroshi.tools\": [\n    {\n      \"tenant\": \"*:r\",\n      \"teams\": [\n        \"*:r\"\n      ]\n    }\n  ]\n}\n```\n\n\n\n\n\n\n\n\n"
  },
  {
    "name": "secure-with-apikey.md",
    "id": "/how-to-s/secure-with-apikey.md",
    "url": "/how-to-s/secure-with-apikey.html",
    "title": "Secure an api with apikeys",
    "content": "# Secure an api with apikeys\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "secure-with-oauth2-client-credentials.md",
    "id": "/how-to-s/secure-with-oauth2-client-credentials.md",
    "url": "/how-to-s/secure-with-oauth2-client-credentials.html",
    "title": "Secure an app with OAuth2 client_credential flow",
    "content": "# Secure an app with OAuth2 client_credential flow\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "setup-otoroshi-cluster.md",
    "id": "/how-to-s/setup-otoroshi-cluster.md",
    "url": "/how-to-s/setup-otoroshi-cluster.html",
    "title": "Setup an Otoroshi cluster",
    "content": "# Setup an Otoroshi cluster\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "tls-using-lets-encrypt.md",
    "id": "/how-to-s/tls-using-lets-encrypt.md",
    "url": "/how-to-s/tls-using-lets-encrypt.html",
    "title": "TLS termination using Let's Encrypt",
    "content": "# TLS termination using Let's Encrypt"
  },
  {
    "name": "index.md",
    "id": "/index.md",
    "url": "/index.html",
    "title": "Otoroshi",
    "content": "# Otoroshi\n\n**Otoroshi** is a layer of lightweight api management on top of a modern http reverse proxy written in <a href=\"https://www.scala-lang.org/\" target=\"_blank\">Scala</a> and developped by the <a href=\"https://maif.github.io\" target=\"_blank\">MAIF OSS</a> team that can handle all the calls to and between your microservices without service locator and let you change configuration dynamicaly at runtime.\n\n\n> *The <a href=\"https://en.wikipedia.org/wiki/Gazu_Hyakki_Yagy%C5%8D#/media/File:SekienOtoroshi.jpg\" target=\"blank\">Otoroshi</a> is a large hairy monster that tends to lurk on the top of the torii gate in front of Shinto shrines. It's a hostile creature, but also said to be the guardian of the shrine and is said to leap down from the top of the gate to devour those who approach the shrine for only self-serving purposes.*\n\n@@@ div { .centered-img }\n[![build](https://github.com/MAIF/otoroshi/actions/workflows/server_build_and_test.yaml/badge.svg)](https://github.com/MAIF/otoroshi/actions/workflows/server_build_and_test.yaml) [![Join the chat at https://gitter.im/MAIF/otoroshi](https://badges.gitter.im/MAIF/otoroshi.svg)](https://gitter.im/MAIF/otoroshi?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [ ![Download](https://img.shields.io/github/release/MAIF/otoroshi.svg) ](hhttps://github.com/MAIF/otoroshi/releases/download/v1.5.0-dev/otoroshi.jar)\n@@@\n\n@@@ div { .centered-img }\n<img src=\"https://github.com/MAIF/otoroshi/raw/master/resources/otoroshi-logo.png\" width=\"300\"></img>\n@@@\n\n## Installation\n\nYou can download the latest build of Otoroshi as a [fat jar](https://github.com/MAIF/otoroshi/releases/download/v1.5.0-dev/otoroshi.jar), as a [zip package](https://github.com/MAIF/otoroshi/releases/download/v1.5.0-dev/otoroshi-dist.zip) or as a [docker image](#).\n\nYou can install and run Otoroshi with this little bash snippet\n\n```sh\ncurl -L -o otoroshi.jar 'https://github.com/MAIF/otoroshi/releases/download/v1.5.0-dev/otoroshi.jar'\njava -jar otoroshi.jar\n```\n\nor using docker\n\n```sh\ndocker run -p \"8080:8080\" maif/otoroshi:1.5.0-dev\n```\n\nnow open your browser to <a href=\"http://otoroshi.oto.tools:8080/\" target=\"_blank\">http://otoroshi.oto.tools:8080/</a>, **log in with the credential generated in the logs** and explore by yourself, if you want better instructions, just go to the [Quick Start](#) or directly to the [installation instructions](#)\n\n## Documentation\n\n* @ref:[About Otoroshi](./about.md)\n* @ref:[Architecture](./architecture.md)\n* @ref:[Features](./features.md)\n* @ref:[Getting started](./getting-started.md)\n* @ref:[Install Otoroshi](./install/index.md)\n* @ref:[Main entities](./entities/index.md)\n* @ref:[Detailed topics](./topics/index.md)\n* @ref:[How to's](./how-to-s/index.md)\n* @ref:[Plugins](./plugins/index.md)\n* @ref:[Admin REST API](./api.md)\n* @ref:[Deploy to production](./deploy/index.md)\n* @ref:[Developing Otoroshi](./dev.md)\n\n## Discussion\n\nJoin the [Otoroshi](https://gitter.im/MAIF/otoroshi) channel on the [MAIF Gitter](https://gitter.im/MAIF)\n\n## Sources\n\nThe sources of Otoroshi are available on [Github](https://github.com/MAIF/otoroshi).\n\n## Logo\n\nYou can find the official Otoroshi logo [on GitHub](https://github.com/MAIF/otoroshi/blob/master/resources/otoroshi-logo.png). The Otoroshi logo has been created by Franois Galioto ([@fgalioto](https://twitter.com/fgalioto))\n\n## Changelog\n\nEvery release, along with the migration instructions, is documented on the [Github Releases](https://github.com/MAIF/otoroshi/releases) page. A condensed version of the changelog is available on [github]((https://github.com/MAIF/otoroshi/CHANGELOG.md)\n\n## Patrons\n\nThe work on Otoroshi was funded by <a href=\"https://www.maif.fr/\" target=\"_blank\">MAIF</a> with the help of the community.\n\n## Licence\n\nOtoroshi is Open Source and available under the [Apache 2 License](https://opensource.org/licenses/Apache-2.0)\n\n@@@ index\n\n* [About Otoroshi](./about.md)\n* [Architecture](./architecture.md)\n* [Features](./features.md)\n* [Getting started](./getting-started.md)\n* [Install Otoroshi](./install/index.md)\n* [Main entities](./entities/index.md)\n* [Detailed topics](./topics/index.md)\n* [How to's](./how-to-s/index.md)\n* [Plugins](./plugins/index.md)\n* [Admin REST API](./api.md)\n* [Deploy to production](./deploy/index.md)\n* [Developing Otoroshi](./dev.md)\n\n@@@\n\n\n@@@ div { .centered-img }\n<!-- oto-scenario\n - goto /bo/dashboard\n - wait 1000\n - screenshot generated-index-md-home.png\n-->\n<img src=\"../imgs/generated-index-md-home.png\" />\n@@@\n"
  },
  {
    "name": "get-otoroshi.md",
    "id": "/install/get-otoroshi.md",
    "url": "/install/get-otoroshi.html",
    "title": "Get Otoroshi",
    "content": "# Get Otoroshi\n\n## From zip\n\n## From jar file\n\n## From Docker\n\n## From Sources\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "index.md",
    "id": "/install/index.md",
    "url": "/install/index.html",
    "title": "Install",
    "content": "# Install\n\nIn this sections, you will find informations about how to install and run Otoroshi\n\n* @ref:[Get Otoroshi](./get-otoroshi.md)\n* @ref:[Setup Otoroshi](./setup-otoroshi.md)\n\n@@@ index\n\n* [Get Otoroshi](./get-otoroshi.md)\n* [Setup Otoroshi](./setup-otoroshi.md)\n\n@@@\n"
  },
  {
    "name": "setup-otoroshi.md",
    "id": "/install/setup-otoroshi.md",
    "url": "/install/setup-otoroshi.html",
    "title": "Setup Otoroshi",
    "content": "# Setup Otoroshi\n\n## Environnement variables\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "built-in-plugins.md",
    "id": "/plugins/built-in-plugins.md",
    "url": "/plugins/built-in-plugins.html",
    "title": "Otoroshi built-in plugins",
    "content": "# Otoroshi built-in plugins\n\nOtoroshi provides some plugins out of the box. Here is the available plugins with their documentation and reference configuration\n\n\n## Access log (CLF)\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `AccessLog`\n\n### Description\n\nWith this plugin, any access to a service will be logged in CLF format.\n\nLog format is the following:\n\n`\"$service\" $clientAddress - \"$userId\" [$timestamp] \"$host $method $path $protocol\" \"$status $statusTxt\" $size $snowflake \"$to\" \"$referer\" \"$userAgent\" $http $duration $errorMsg`\n\nThe plugin accepts the following configuration\n\n```json\n{\n  \"AccessLog\": {\n    \"enabled\": true,\n    \"statuses\": [], // list of status to enable logs, if none, log everything\n    \"paths\": [], // list of paths to enable logs, if none, log everything\n    \"methods\": [], // list of http methods to enable logs, if none, log everything\n    \"identities\": [] // list of identities to enable logs, if none, log everything\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"AccessLog\" : {\n    \"enabled\" : true,\n    \"statuses\" : [ ],\n    \"paths\" : [ ],\n    \"methods\" : [ ],\n    \"identities\" : [ ]\n  }\n}\n```\n\n\n\n\n\n\n## Access log (JSON)\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `AccessLog`\n\n### Description\n\nWith this plugin, any access to a service will be logged in json format.\n\nThe plugin accepts the following configuration\n\n```json\n{\n  \"AccessLog\": {\n    \"enabled\": true,\n    \"statuses\": [], // list of status to enable logs, if none, log everything\n    \"paths\": [], // list of paths to enable logs, if none, log everything\n    \"methods\": [], // list of http methods to enable logs, if none, log everything\n    \"identities\": [] // list of identities to enable logs, if none, log everything\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"AccessLog\" : {\n    \"enabled\" : true,\n    \"statuses\" : [ ],\n    \"paths\" : [ ],\n    \"methods\" : [ ],\n    \"identities\" : [ ]\n  }\n}\n```\n\n\n\n\n\n\n## Kafka access log\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `KafkaAccessLog`\n\n### Description\n\nWith this plugin, any access to a service will be logged as an event in a kafka topic.\n\nThe plugin accepts the following configuration\n\n```json\n{\n  \"KafkaAccessLog\": {\n    \"enabled\": true,\n    \"topic\": \"otoroshi-access-log\",\n    \"statuses\": [], // list of status to enable logs, if none, log everything\n    \"paths\": [], // list of paths to enable logs, if none, log everything\n    \"methods\": [], // list of http methods to enable logs, if none, log everything\n    \"identities\": [] // list of identities to enable logs, if none, log everything\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"KafkaAccessLog\" : {\n    \"enabled\" : true,\n    \"topic\" : \"otoroshi-access-log\",\n    \"statuses\" : [ ],\n    \"paths\" : [ ],\n    \"methods\" : [ ],\n    \"identities\" : [ ]\n  }\n}\n```\n\n\n\n\n\n\n## Basic Auth. caller\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `BasicAuthCaller`\n\n### Description\n\nThis plugin can be used to call api that are authenticated using basic auth.\n\nThis plugin accepts the following configuration\n\n{\n  \"username\" : \"the_username\",\n  \"password\" : \"the_password\",\n  \"headerName\" : \"Authorization\",\n  \"headerValueFormat\" : \"Basic %s\"\n}\n\n\n\n### Default configuration\n\n```json\n{\n  \"username\" : \"the_username\",\n  \"password\" : \"the_password\",\n  \"headerName\" : \"Authorization\",\n  \"headerValueFormat\" : \"Basic %s\"\n}\n```\n\n\n\n\n\n\n## OAuth2 caller\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `OAuth2Caller`\n\n### Description\n\nThis plugin can be used to call api that are authenticated using OAuth2 client_credential/password flow.\nDo not forget to enable client retry to handle token generation on expire.\n\nThis plugin accepts the following configuration\n\n{\n  \"kind\" : \"the oauth2 flow, can be 'client_credentials' or 'password'\",\n  \"url\" : \"https://127.0.0.1:8080/oauth/token\",\n  \"method\" : \"POST\",\n  \"headerName\" : \"Authorization\",\n  \"headerValueFormat\" : \"Bearer %s\",\n  \"jsonPayload\" : false,\n  \"clientId\" : \"the client_id\",\n  \"clientSecret\" : \"the client_secret\",\n  \"scope\" : \"an optional scope\",\n  \"audience\" : \"an optional audience\",\n  \"user\" : \"an optional username if using password flow\",\n  \"password\" : \"an optional password if using password flow\",\n  \"cacheTokenSeconds\" : \"the number of second to wait before asking for a new token\",\n  \"tlsConfig\" : \"an optional TLS settings object\"\n}\n\n\n\n### Default configuration\n\n```json\n{\n  \"kind\" : \"the oauth2 flow, can be 'client_credentials' or 'password'\",\n  \"url\" : \"https://127.0.0.1:8080/oauth/token\",\n  \"method\" : \"POST\",\n  \"headerName\" : \"Authorization\",\n  \"headerValueFormat\" : \"Bearer %s\",\n  \"jsonPayload\" : false,\n  \"clientId\" : \"the client_id\",\n  \"clientSecret\" : \"the client_secret\",\n  \"scope\" : \"an optional scope\",\n  \"audience\" : \"an optional audience\",\n  \"user\" : \"an optional username if using password flow\",\n  \"password\" : \"an optional password if using password flow\",\n  \"cacheTokenSeconds\" : \"the number of second to wait before asking for a new token\",\n  \"tlsConfig\" : \"an optional TLS settings object\"\n}\n```\n\n\n\n\n\n\n## Response Cache\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `ResponseCache`\n\n### Description\n\nThis plugin can cache responses from target services in the otoroshi datasstore\nIt also provides a debug UI at `/.well-known/otoroshi/bodylogger`.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"ResponseCache\": {\n    \"enabled\": true, // enabled cache\n    \"ttl\": 300000,  // store it for some times (5 minutes by default)\n    \"maxSize\": 5242880, // max body size (body will be cut after that)\n    \"autoClean\": true, // cleanup older keys when all bigger than maxSize\n    \"filter\": { // cache only for some status, method and paths\n      \"statuses\": [],\n      \"methods\": [],\n      \"paths\": [],\n      \"not\": {\n        \"statuses\": [],\n        \"methods\": [],\n        \"paths\": []\n      }\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"ResponseCache\" : {\n    \"enabled\" : true,\n    \"ttl\" : 3600000,\n    \"maxSize\" : 52428800,\n    \"autoClean\" : true,\n    \"filter\" : {\n      \"statuses\" : [ ],\n      \"methods\" : [ ],\n      \"paths\" : [ ],\n      \"not\" : {\n        \"statuses\" : [ ],\n        \"methods\" : [ ],\n        \"paths\" : [ ]\n      }\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Client certificate header\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `ClientCertChain`\n\n### Description\n\nThis plugin pass client certificate informations to the target in headers.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"ClientCertChain\": {\n    \"pem\": { // send client cert as PEM format in a header\n      \"send\": false,\n      \"header\": \"X-Client-Cert-Pem\"\n    },\n    \"dns\": { // send JSON array of DNs in a header\n      \"send\": false,\n      \"header\": \"X-Client-Cert-DNs\"\n    },\n    \"chain\": { // send JSON representation of client cert chain in a header\n      \"send\": true,\n      \"header\": \"X-Client-Cert-Chain\"\n    },\n    \"claims\": { // pass JSON representation of client cert chain in the otoroshi JWT token\n      \"send\": false,\n      \"name\": \"clientCertChain\"\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"ClientCertChain\" : {\n    \"pem\" : {\n      \"send\" : false,\n      \"header\" : \"X-Client-Cert-Pem\"\n    },\n    \"dns\" : {\n      \"send\" : false,\n      \"header\" : \"X-Client-Cert-DNs\"\n    },\n    \"chain\" : {\n      \"send\" : true,\n      \"header\" : \"X-Client-Cert-Chain\"\n    },\n    \"claims\" : {\n      \"send\" : false,\n      \"name\" : \"clientCertChain\"\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Defer Responses\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `DeferPlugin`\n\n### Description\n\nThis plugin will expect a `X-Defer` header or a `defer` query param and defer the response according to the value in milliseconds.\nThis plugin is some kind of inside joke as one a our customer ask us to make slower apis.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"DeferPlugin\": {\n    \"defaultDefer\": 0 // default defer in millis\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"DeferPlugin\" : {\n    \"defaultDefer\" : 0\n  }\n}\n```\n\n\n\n\n\n\n## Self registration endpoints (service discovery)\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `DiscoverySelfRegistration`\n\n### Description\n\nThis plugin add support for self registration endpoint on a specific service.\n\nThis plugin accepts the following configuration:\n\n\n\n### Default configuration\n\n```json\n{\n  \"DiscoverySelfRegistration\" : {\n    \"hosts\" : [ ],\n    \"targetTemplate\" : { },\n    \"registrationTtl\" : 60000\n  }\n}\n```\n\n\n\n\n\n\n## Envoy Control Plane (experimental)\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `EnvoyControlPlane`\n\n### Description\n\nThis plugin will expose the otoroshi state to envoy instances using the xDS V3 API`.\n\nRight now, all the features of otoroshi cannot be exposed as is through Envoy.\n\n\n\n### Default configuration\n\n```json\n{\n  \"EnvoyControlPlane\" : {\n    \"enabled\" : true\n  }\n}\n```\n\n\n\n\n\n\n## Geolocation endpoint\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: ``none``\n\n### Description\n\nThis plugin will expose current geolocation informations on the following endpoint.\n\n`/.well-known/otoroshi/plugins/geolocation`\n\n\n\n\n\n\n\n\n## Geolocation header\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `GeolocationInfoHeader`\n\n### Description\n\nThis plugin will send informations extracted by the Geolocation details extractor to the target service in a header.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"GeolocationInfoHeader\": {\n    \"headerName\": \"X-Geolocation-Info\" // header in which info will be sent\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"GeolocationInfoHeader\" : {\n    \"headerName\" : \"X-Geolocation-Info\"\n  }\n}\n```\n\n\n\n\n\n\n## Izanami Canary Campaign\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `IzanamiCanary`\n\n### Description\n\nThis plugin allow you to perform canary testing based on an izanami experiment campaign (A/B test).\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"IzanamiCanary\" : {\n    \"experimentId\" : \"foo:bar:qix\",\n    \"configId\" : \"foo:bar:qix:config\",\n    \"izanamiUrl\" : \"https://izanami.foo.bar\",\n    \"izanamiClientId\" : \"client\",\n    \"izanamiClientSecret\" : \"secret\",\n    \"timeout\" : 5000,\n    \"mtls\" : {\n      \"certs\" : [ ],\n      \"trustedCerts\" : [ ],\n      \"mtls\" : false,\n      \"loose\" : false,\n      \"trustAll\" : false\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"IzanamiCanary\" : {\n    \"experimentId\" : \"foo:bar:qix\",\n    \"configId\" : \"foo:bar:qix:config\",\n    \"izanamiUrl\" : \"https://izanami.foo.bar\",\n    \"izanamiClientId\" : \"client\",\n    \"izanamiClientSecret\" : \"secret\",\n    \"timeout\" : 5000,\n    \"mtls\" : {\n      \"certs\" : [ ],\n      \"trustedCerts\" : [ ],\n      \"mtls\" : false,\n      \"loose\" : false,\n      \"trustAll\" : false\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Izanami APIs Proxy\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `IzanamiProxy`\n\n### Description\n\nThis plugin exposes routes to proxy Izanami configuration and features tree APIs.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"IzanamiProxy\" : {\n    \"path\" : \"/api/izanami\",\n    \"featurePattern\" : \"*\",\n    \"configPattern\" : \"*\",\n    \"autoContext\" : false,\n    \"featuresEnabled\" : true,\n    \"featuresWithContextEnabled\" : true,\n    \"configurationEnabled\" : false,\n    \"izanamiUrl\" : \"https://izanami.foo.bar\",\n    \"izanamiClientId\" : \"client\",\n    \"izanamiClientSecret\" : \"secret\",\n    \"timeout\" : 5000\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"IzanamiProxy\" : {\n    \"path\" : \"/api/izanami\",\n    \"featurePattern\" : \"*\",\n    \"configPattern\" : \"*\",\n    \"autoContext\" : false,\n    \"featuresEnabled\" : true,\n    \"featuresWithContextEnabled\" : true,\n    \"configurationEnabled\" : false,\n    \"izanamiUrl\" : \"https://izanami.foo.bar\",\n    \"izanamiClientId\" : \"client\",\n    \"izanamiClientSecret\" : \"secret\",\n    \"timeout\" : 5000\n  }\n}\n```\n\n\n\n\n\n\n## Html Patcher\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `HtmlPatcher`\n\n### Description\n\nThis plugin can inject elements in html pages (in the body or in the head) returned by the service\n\n\n\n### Default configuration\n\n```json\n{\n  \"HtmlPatcher\" : {\n    \"appendHead\" : [ ],\n    \"appendBody\" : [ ]\n  }\n}\n```\n\n\n\n\n\n\n## Body logger\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `BodyLogger`\n\n### Description\n\nThis plugin can log body present in request and response. It can just logs it, store in in the redis store with a ttl and send it to analytics.\nIt also provides a debug UI at `/.well-known/otoroshi/bodylogger`.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"BodyLogger\": {\n    \"enabled\": true, // enabled logging\n    \"log\": true, // just log it\n    \"store\": false, // store bodies in datastore\n    \"ttl\": 300000,  // store it for some times (5 minutes by default)\n    \"sendToAnalytics\": false, // send bodies to analytics\n    \"maxSize\": 5242880, // max body size (body will be cut after that)\n    \"password\": \"password\", // password for the ui, if none, it's public\n    \"filter\": { // log only for some status, method and paths\n      \"statuses\": [],\n      \"methods\": [],\n      \"paths\": [],\n      \"not\": {\n        \"statuses\": [],\n        \"methods\": [],\n        \"paths\": []\n      }\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"BodyLogger\" : {\n    \"enabled\" : true,\n    \"log\" : true,\n    \"store\" : false,\n    \"ttl\" : 300000,\n    \"sendToAnalytics\" : false,\n    \"maxSize\" : 5242880,\n    \"password\" : \"password\",\n    \"filter\" : {\n      \"statuses\" : [ ],\n      \"methods\" : [ ],\n      \"paths\" : [ ],\n      \"not\" : {\n        \"statuses\" : [ ],\n        \"methods\" : [ ],\n        \"paths\" : [ ]\n      }\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Prometheus Service Metrics\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `PrometheusServiceMetrics`\n\n### Description\n\nThis plugin collects service metrics and can be used with the `Prometheus Endpoint` (in the Danger Zone) plugin to expose those metrics\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"PrometheusServiceMetrics\": {\n    \"includeUri\": false // include http uri in metrics. WARNING this could impliess performance issues, use at your own risks\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"PrometheusServiceMetrics\" : {\n    \"includeUri\" : false\n  }\n}\n```\n\n\n\n\n\n\n## Service Metrics\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `ServiceMetrics`\n\n### Description\n\nThis plugin expose service metrics in Otoroshi global metrics or on a special URL of the service `/.well-known/otoroshi/metrics`.\nMetrics are exposed in json or prometheus format depending on the accept header. You can protect it with an access key defined in the configuration\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"ServiceMetrics\": {\n    \"accessKeyValue\": \"secret\", // if not defined, public access. Can be ${config.app.health.accessKey}\n    \"accessKeyQuery\": \"access_key\"\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"ServiceMetrics\" : {\n    \"accessKeyValue\" : \"${config.app.health.accessKey}\",\n    \"accessKeyQuery\" : \"access_key\"\n  }\n}\n```\n\n\n\n\n\n\n## Mirroring plugin\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `MirroringPlugin`\n\n### Description\n\nThis plugin will mirror every request to other targets\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"MirroringPlugin\": {\n    \"enabled\": true, // enabled mirroring\n    \"to\": \"https://foo.bar.dev\", // the url of the service to mirror\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"MirroringPlugin\" : {\n    \"enabled\" : true,\n    \"to\" : \"https://foo.bar.dev\",\n    \"captureResponse\" : false,\n    \"generateEvents\" : false\n  }\n}\n```\n\n\n\n\n\n\n## OIDC headers\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `OIDCHeaders`\n\n### Description\n\nThis plugin injects headers containing tokens and profile from current OIDC provider.\n\n\n\n### Default configuration\n\n```json\n{\n  \"OIDCHeaders\" : {\n    \"profile\" : {\n      \"send\" : true,\n      \"headerName\" : \"X-OIDC-User\"\n    },\n    \"idtoken\" : {\n      \"send\" : false,\n      \"name\" : \"id_token\",\n      \"headerName\" : \"X-OIDC-Id-Token\",\n      \"jwt\" : true\n    },\n    \"accesstoken\" : {\n      \"send\" : false,\n      \"name\" : \"access_token\",\n      \"headerName\" : \"X-OIDC-Access-Token\",\n      \"jwt\" : true\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Security Txt\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `SecurityTxt`\n\n### Description\n\nThis plugin exposes a special route `/.well-known/security.txt` as proposed at [https://securitytxt.org/](https://securitytxt.org/).\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"SecurityTxt\": {\n    \"Contact\": \"contact@foo.bar\", // mandatory, a link or e-mail address for people to contact you about security issues\n    \"Encryption\": \"http://url-to-public-key\", // optional, a link to a key which security researchers should use to securely talk to you\n    \"Acknowledgments\": \"http://url\", // optional, a link to a web page where you say thank you to security researchers who have helped you\n    \"Preferred-Languages\": \"en, fr, es\", // optional\n    \"Policy\": \"http://url\", // optional, a link to a policy detailing what security researchers should do when searching for or reporting security issues\n    \"Hiring\": \"http://url\", // optional, a link to any security-related job openings in your organisation\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"SecurityTxt\" : {\n    \"Contact\" : \"contact@foo.bar\",\n    \"Encryption\" : \"https://...\",\n    \"Acknowledgments\" : \"https://...\",\n    \"Preferred-Languages\" : \"en, fr\",\n    \"Policy\" : \"https://...\",\n    \"Hiring\" : \"https://...\"\n  }\n}\n```\n\n\n\n\n\n\n## Static Response\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `StaticResponse`\n\n### Description\n\nThis plugin returns a static response for any request\n\n\n\n### Default configuration\n\n```json\n{\n  \"StaticResponse\" : {\n    \"status\" : 200,\n    \"headers\" : {\n      \"Content-Type\" : \"application/json\"\n    },\n    \"body\" : \"{\\\"message\\\":\\\"hello world!\\\"}\",\n    \"bodyBase64\" : null\n  }\n}\n```\n\n\n\n\n\n\n## User-Agent endpoint\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: ``none``\n\n### Description\n\nThis plugin will expose current user-agent informations on the following endpoint.\n\n`/.well-known/otoroshi/plugins/user-agent`\n\n\n\n\n\n\n\n\n## User-Agent header\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `UserAgentInfoHeader`\n\n### Description\n\nThis plugin will sent informations extracted by the User-Agent details extractor to the target service in a header.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"UserAgentInfoHeader\": {\n    \"headerName\": \"X-User-Agent-Info\" // header in which info will be sent\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"UserAgentInfoHeader\" : {\n    \"headerName\" : \"X-User-Agent-Info\"\n  }\n}\n```\n\n\n\n\n\n\n## Workflow endpoint\n\n### Infos\n\n* plugin type: `transformer`\n* configuration root: `WorkflowEndpoint`\n\n### Description\n\nThis plugin runs a workflow and return the response\n\n\n\n### Default configuration\n\n```json\n{\n  \"WorkflowEndpoint\" : {\n    \"workflow\" : { }\n  }\n}\n```\n\n\n\n\n\n\n## Biscuit token validator\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: ``none``\n\n### Description\n\nThis plugin validates a Biscuit token.\n\n\n\n### Default configuration\n\n```json\n{\n  \"publicKey\" : \"xxxxxx\",\n  \"secret\" : \"secret\",\n  \"checks\" : [ ],\n  \"facts\" : [ ],\n  \"resources\" : [ ],\n  \"rules\" : [ ],\n  \"revocation_ids\" : [ ],\n  \"enforce\" : false,\n  \"sealed\" : false,\n  \"extractor\" : {\n    \"type\" : \"header\",\n    \"name\" : \"Authorization\"\n  }\n}\n```\n\n\n\n\n\n\n## Client Certificate + Api Key only\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: ``none``\n\n### Description\n\nCheck if a client certificate is present in the request and that the apikey used matches the client certificate.\nYou can set the client cert. DN in an apikey metadata named `allowed-client-cert-dn`\n\n\n\n\n\n\n\n\n## Client certificate matching (over http)\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: `HasClientCertMatchingHttpValidator`\n\n### Description\n\nCheck if client certificate matches the following configuration\n\nexpected response from http service is\n\n```json\n{\n  \"serialNumbers\": [],   // allowed certificated serial numbers\n  \"subjectDNs\": [],      // allowed certificated DNs\n  \"issuerDNs\": [],       // allowed certificated issuer DNs\n  \"regexSubjectDNs\": [], // allowed certificated DNs matching regex\n  \"regexIssuerDNs\": [],  // allowed certificated issuer DNs matching regex\n}\n```\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"HasClientCertMatchingValidator\": {\n    \"url\": \"...\",   // url for the call\n    \"headers\": {},  // http header for the call\n    \"ttl\": 600000,  // cache ttl,\n    \"mtlsConfig\": {\n      \"certId\": \"xxxxx\",\n       \"mtls\": false,\n       \"loose\": false\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"HasClientCertMatchingHttpValidator\" : {\n    \"url\" : \"http://foo.bar\",\n    \"ttl\" : 600000,\n    \"headers\" : { },\n    \"mtlsConfig\" : {\n      \"certId\" : \"...\",\n      \"mtls\" : false,\n      \"loose\" : false\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Client certificate matching\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: `HasClientCertMatchingValidator`\n\n### Description\n\nCheck if client certificate matches the following configuration\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"HasClientCertMatchingValidator\": {\n    \"serialNumbers\": [],   // allowed certificated serial numbers\n    \"subjectDNs\": [],      // allowed certificated DNs\n    \"issuerDNs\": [],       // allowed certificated issuer DNs\n    \"regexSubjectDNs\": [], // allowed certificated DNs matching regex\n    \"regexIssuerDNs\": [],  // allowed certificated issuer DNs matching regex\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"HasClientCertMatchingValidator\" : {\n    \"serialNumbers\" : [ ],\n    \"subjectDNs\" : [ ],\n    \"issuerDNs\" : [ ],\n    \"regexSubjectDNs\" : [ ],\n    \"regexIssuerDNs\" : [ ]\n  }\n}\n```\n\n\n\n\n\n\n## Client Certificate Only\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: ``none``\n\n### Description\n\nCheck if a client certificate is present in the request\n\n\n\n\n\n\n\n\n## External Http Validator\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: `ExternalHttpValidator`\n\n### Description\n\nCalls an external http service to know if a user has access or not. Uses cache for performances.\n\nThe sent payload is the following:\n\n```json\n{\n  \"apikey\": {...},\n  \"user\": {...},\n  \"service\": : {...},\n  \"chain\": \"...\",  // PEM cert chain\n  \"fingerprints\": [...]\n}\n```\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"ExternalHttpValidator\": {\n    \"url\": \"...\",                      // url for the http call\n    \"host\": \"...\",                     // value of the host header for the call. default is host of the url\n    \"goodTtl\": 600000,                 // ttl in ms for a validated call\n    \"badTtl\": 60000,                   // ttl in ms for a not validated call\n    \"method\": \"POST\",                  // http methode\n    \"path\": \"/certificates/_validate\", // http uri path\n    \"timeout\": 10000,                  // http call timeout\n    \"noCache\": false,                  // use cache or not\n    \"allowNoClientCert\": false,        //\n    \"headers\": {},                      // headers for the http call if needed\n    \"mtlsConfig\": {\n      \"certId\": \"xxxxx\",\n       \"mtls\": false,\n       \"loose\": false\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"ExternalHttpValidator\" : {\n    \"url\" : \"http://foo.bar\",\n    \"host\" : \"api.foo.bar\",\n    \"goodTtl\" : 600000,\n    \"badTtl\" : 60000,\n    \"method\" : \"POST\",\n    \"path\" : \"/certificates/_validate\",\n    \"timeout\" : 10000,\n    \"noCache\" : false,\n    \"allowNoClientCert\" : false,\n    \"headers\" : { },\n    \"mtlsConfig\" : {\n      \"certId\" : \"...\",\n      \"mtls\" : false,\n      \"loose\" : false\n    }\n  }\n}\n```\n\n\n\n\n\n\n## OIDC access_token validator\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: `OIDCAccessTokenValidator`\n\n### Description\n\nThis plugin will use the third party apikey configuration and apply it while keeping the apikey mecanism of otoroshi.\nUse it to combine apikey validation and OIDC access_token validation.\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"OIDCAccessTokenValidator\": {\n    \"enabled\": true,\n    \"atLeastOne\": false,\n    // config is optional and can be either an object config or an array of objects\n    \"config\": {\n  \"enabled\" : true,\n  \"quotasEnabled\" : true,\n  \"uniqueApiKey\" : false,\n  \"type\" : \"OIDC\",\n  \"oidcConfigRef\" : \"some-oidc-auth-module-id\",\n  \"localVerificationOnly\" : false,\n  \"mode\" : \"Tmp\",\n  \"ttl\" : 0,\n  \"headerName\" : \"Authorization\",\n  \"throttlingQuota\" : 100,\n  \"dailyQuota\" : 10000000,\n  \"monthlyQuota\" : 10000000,\n  \"excludedPatterns\" : [ ],\n  \"scopes\" : [ ],\n  \"rolesPath\" : [ ],\n  \"roles\" : [ ]\n}\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"OIDCAccessTokenValidator\" : {\n    \"enabled\" : true,\n    \"atLeastOne\" : false,\n    \"config\" : {\n      \"enabled\" : true,\n      \"quotasEnabled\" : true,\n      \"uniqueApiKey\" : false,\n      \"type\" : \"OIDC\",\n      \"oidcConfigRef\" : \"some-oidc-auth-module-id\",\n      \"localVerificationOnly\" : false,\n      \"mode\" : \"Tmp\",\n      \"ttl\" : 0,\n      \"headerName\" : \"Authorization\",\n      \"throttlingQuota\" : 100,\n      \"dailyQuota\" : 10000000,\n      \"monthlyQuota\" : 10000000,\n      \"excludedPatterns\" : [ ],\n      \"scopes\" : [ ],\n      \"rolesPath\" : [ ],\n      \"roles\" : [ ]\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Instance quotas\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: `InstanceQuotas`\n\n### Description\n\nThis plugin will enforce global quotas on the current instance\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"InstanceQuotas\": {\n    \"callsPerDay\": -1,     // max allowed api calls per day\n    \"callsPerMonth\": -1,   // max allowed api calls per month\n    \"maxDescriptors\": -1,  // max allowed service descriptors\n    \"maxApiKeys\": -1,      // max allowed apikeys\n    \"maxGroups\": -1,       // max allowed service groups\n    \"maxScripts\": -1,      // max allowed apikeys\n    \"maxCertificates\": -1, // max allowed certificates\n    \"maxVerifiers\": -1,    // max allowed jwt verifiers\n    \"maxAuthModules\": -1,  // max allowed auth modules\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"InstanceQuotas\" : {\n    \"callsPerDay\" : -1,\n    \"callsPerMonth\" : -1,\n    \"maxDescriptors\" : -1,\n    \"maxApiKeys\" : -1,\n    \"maxGroups\" : -1,\n    \"maxScripts\" : -1,\n    \"maxCertificates\" : -1,\n    \"maxVerifiers\" : -1,\n    \"maxAuthModules\" : -1\n  }\n}\n```\n\n\n\n\n\n\n## Public quotas\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: `ServiceQuotas`\n\n### Description\n\nThis plugin will enforce public quotas on the current service\n\n\n\n\n\n\n\n### Default configuration\n\n```json\n{\n  \"ServiceQuotas\" : {\n    \"throttlingQuota\" : 100,\n    \"dailyQuota\" : 10000000,\n    \"monthlyQuota\" : 10000000\n  }\n}\n```\n\n\n\n\n\n\n## Allowed users only\n\n### Infos\n\n* plugin type: `validator`\n* configuration root: `HasAllowedUsersValidator`\n\n### Description\n\nThis plugin only let allowed users pass\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"HasAllowedUsersValidator\": {\n    \"usernames\": [],   // allowed usernames\n    \"emails\": [],      // allowed user email addresses\n    \"emailDomains\": [], // allowed user email domains\n    \"metadataMatch\": [], // json path expressions to match against user metadata. passes if one match\n    \"metadataNotMatch\": [], // json path expressions to match against user metadata. passes if none match\n    \"profileMatch\": [], // json path expressions to match against user profile. passes if one match\n    \"profileNotMatch\": [], // json path expressions to match against user profile. passes if none match\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"HasAllowedUsersValidator\" : {\n    \"usernames\" : [ ],\n    \"emails\" : [ ],\n    \"emailDomains\" : [ ],\n    \"metadataMatch\" : [ ],\n    \"metadataNotMatch\" : [ ],\n    \"profileMatch\" : [ ],\n    \"profileNotMatch\" : [ ]\n  }\n}\n```\n\n\n\n\n\n\n## Apikey auth module\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `ApikeyAuthModule`\n\n### Description\n\nThis plugin adds basic auth on service where credentials are valid apikeys on the current service.\n\n\n\n### Default configuration\n\n```json\n{\n  \"ApikeyAuthModule\" : {\n    \"realm\" : \"apikey-auth-module-realm\",\n    \"noneTagIn\" : [ ],\n    \"oneTagIn\" : [ ],\n    \"allTagsIn\" : [ ],\n    \"noneMetaIn\" : [ ],\n    \"oneMetaIn\" : [ ],\n    \"allMetaIn\" : [ ],\n    \"noneMetaKeysIn\" : [ ],\n    \"oneMetaKeyIn\" : [ ],\n    \"allMetaKeysIn\" : [ ]\n  }\n}\n```\n\n\n\n\n\n\n## Client certificate as apikey\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `CertificateAsApikey`\n\n### Description\n\nThis plugin uses client certificate as an apikey. The apikey will be stored for classic apikey usage\n\n\n\n### Default configuration\n\n```json\n{\n  \"CertificateAsApikey\" : {\n    \"readOnly\" : false,\n    \"allowClientIdOnly\" : false,\n    \"throttlingQuota\" : 100,\n    \"dailyQuota\" : 10000000,\n    \"monthlyQuota\" : 10000000,\n    \"constrainedServicesOnly\" : false,\n    \"tags\" : [ ],\n    \"metadata\" : { }\n  }\n}\n```\n\n\n\n\n\n\n## Client Credential Flow ApiKey extractor\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: ``none``\n\n### Description\n\nThis plugin can extract an apikey from an opaque access_token generate by the `ClientCredentialFlow` plugin\n\n\n\n\n\n\n\n\n## Apikey from Biscuit token extractor\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: ``none``\n\n### Description\n\nThis plugin extract an from a Biscuit token where the biscuit has an #authority fact 'client_id' containing\napikey client_id and an #authority fact 'client_sign' that is the HMAC256 signature of the apikey client_id with the apikey client_secret\n\n\n\n### Default configuration\n\n```json\n{\n  \"publicKey\" : \"xxxxxx\",\n  \"secret\" : \"secret\",\n  \"checks\" : [ ],\n  \"facts\" : [ ],\n  \"resources\" : [ ],\n  \"rules\" : [ ],\n  \"revocation_ids\" : [ ],\n  \"enforce\" : false,\n  \"sealed\" : false,\n  \"extractor\" : {\n    \"type\" : \"header\",\n    \"name\" : \"Authorization\"\n  }\n}\n```\n\n\n\n\n\n\n## Service discovery target selector (service discovery)\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `DiscoverySelfRegistration`\n\n### Description\n\nThis plugin select a target in the pool of discovered targets for this service.\nUse in combination with either `DiscoverySelfRegistrationSink` or `DiscoverySelfRegistrationTransformer` to make it work using the `self registration` pattern.\nOr use an implementation of `DiscoveryJob` for the `third party registration pattern`.\n\nThis plugin accepts the following configuration:\n\n\n\n### Default configuration\n\n```json\n{\n  \"DiscoverySelfRegistration\" : {\n    \"hosts\" : [ ],\n    \"targetTemplate\" : { },\n    \"registrationTtl\" : 60000\n  }\n}\n```\n\n\n\n\n\n\n## Geolocation details extractor (using IpStack api)\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `GeolocationInfo`\n\n### Description\n\nThis plugin extract geolocation informations from ip address using the [IpStack dbs](https://ipstack.com/).\nThe informations are store in plugins attrs for other plugins to use\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"GeolocationInfo\": {\n    \"apikey\": \"xxxxxxx\",\n    \"timeout\": 2000, // timeout in ms\n    \"log\": false // will log geolocation details\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"GeolocationInfo\" : {\n    \"apikey\" : \"xxxxxxx\",\n    \"timeout\" : 2000,\n    \"log\" : false\n  }\n}\n```\n\n\n\n\n\n\n## Geolocation details extractor (using Maxmind db)\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `GeolocationInfo`\n\n### Description\n\nThis plugin extract geolocation informations from ip address using the [Maxmind dbs](https://www.maxmind.com/en/geoip2-databases).\nThe informations are store in plugins attrs for other plugins to use\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"GeolocationInfo\": {\n    \"path\": \"/foo/bar/cities.mmdb\", // file path, can be \"global\"\n    \"log\": false // will log geolocation details\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"GeolocationInfo\" : {\n    \"path\" : \"global\",\n    \"log\" : false\n  }\n}\n```\n\n\n\n\n\n\n## Jwt user extractor\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `JwtUserExtractor`\n\n### Description\n\nThis plugin extract a user from a JWT token\n\n\n\n### Default configuration\n\n```json\n{\n  \"JwtUserExtractor\" : {\n    \"verifier\" : \"\",\n    \"strict\" : true,\n    \"namePath\" : \"name\",\n    \"emailPath\" : \"email\",\n    \"metaPath\" : null\n  }\n}\n```\n\n\n\n\n\n\n## OIDC access_token as apikey\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `OIDCAccessTokenAsApikey`\n\n### Description\n\nThis plugin will use the third party apikey configuration to generate an apikey\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"OIDCAccessTokenValidator\": {\n    \"enabled\": true,\n    \"atLeastOne\": false,\n    // config is optional and can be either an object config or an array of objects\n    \"config\": {\n  \"enabled\" : true,\n  \"quotasEnabled\" : true,\n  \"uniqueApiKey\" : false,\n  \"type\" : \"OIDC\",\n  \"oidcConfigRef\" : \"some-oidc-auth-module-id\",\n  \"localVerificationOnly\" : false,\n  \"mode\" : \"Tmp\",\n  \"ttl\" : 0,\n  \"headerName\" : \"Authorization\",\n  \"throttlingQuota\" : 100,\n  \"dailyQuota\" : 10000000,\n  \"monthlyQuota\" : 10000000,\n  \"excludedPatterns\" : [ ],\n  \"scopes\" : [ ],\n  \"rolesPath\" : [ ],\n  \"roles\" : [ ]\n}\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"OIDCAccessTokenAsApikey\" : {\n    \"enabled\" : true,\n    \"atLeastOne\" : false,\n    \"config\" : {\n      \"enabled\" : true,\n      \"quotasEnabled\" : true,\n      \"uniqueApiKey\" : false,\n      \"type\" : \"OIDC\",\n      \"oidcConfigRef\" : \"some-oidc-auth-module-id\",\n      \"localVerificationOnly\" : false,\n      \"mode\" : \"Tmp\",\n      \"ttl\" : 0,\n      \"headerName\" : \"Authorization\",\n      \"throttlingQuota\" : 100,\n      \"dailyQuota\" : 10000000,\n      \"monthlyQuota\" : 10000000,\n      \"excludedPatterns\" : [ ],\n      \"scopes\" : [ ],\n      \"rolesPath\" : [ ],\n      \"roles\" : [ ]\n    }\n  }\n}\n```\n\n\n\n\n\n\n## User-Agent details extractor\n\n### Infos\n\n* plugin type: `preroute`\n* configuration root: `UserAgentInfo`\n\n### Description\n\nThis plugin extract informations from User-Agent header such as browsser version, OS version, etc.\nThe informations are store in plugins attrs for other plugins to use\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"UserAgentInfo\": {\n    \"log\": false // will log user-agent details\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"UserAgentInfo\" : {\n    \"log\" : false\n  }\n}\n```\n\n\n\n\n\n\n## Client Credential Service\n\n### Infos\n\n* plugin type: `sink`\n* configuration root: `ClientCredentialService`\n\n### Description\n\nThis plugin add an an oauth client credentials service (`https://unhandleddomain/.well-known/otoroshi/oauth/token`) to create an access_token given a client id and secret.\n\n```json\n{\n  \"ClientCredentialService\" : {\n    \"domain\" : \"*\",\n    \"expiration\" : 3600000,\n    \"defaultKeyPair\" : \"otoroshi-jwt-signing\",\n    \"secure\" : true\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"ClientCredentialService\" : {\n    \"domain\" : \"*\",\n    \"expiration\" : 3600000,\n    \"defaultKeyPair\" : \"otoroshi-jwt-signing\",\n    \"secure\" : true\n  }\n}\n```\n\n\n\n\n\n\n## Global self registration endpoints (service discovery)\n\n### Infos\n\n* plugin type: `sink`\n* configuration root: `DiscoverySelfRegistration`\n\n### Description\n\nThis plugin add support for self registration endpoint on specific hostnames.\n\nThis plugin accepts the following configuration:\n\n\n\n### Default configuration\n\n```json\n{\n  \"DiscoverySelfRegistration\" : {\n    \"hosts\" : [ ],\n    \"targetTemplate\" : { },\n    \"registrationTtl\" : 60000\n  }\n}\n```\n\n\n\n\n\n\n## Kubernetes admission validator webhook\n\n### Infos\n\n* plugin type: `sink`\n* configuration root: ``none``\n\n### Description\n\nThis plugin exposes a webhook to kubernetes to handle manifests validation\n\n\n\n\n\n\n\n\n## Kubernetes sidecar injector webhook\n\n### Infos\n\n* plugin type: `sink`\n* configuration root: ``none``\n\n### Description\n\nThis plugin exposes a webhook to kubernetes to inject otoroshi-sidecar in pods\n\n\n\n\n\n\n\n\n## Prometheus Endpoint\n\n### Infos\n\n* plugin type: `sink`\n* configuration root: `PrometheusEndpoint`\n\n### Description\n\nThis plugin exposes metrics collected by `Prometheus Service Metrics` on a `/prometheus` endpoint.\nYou can protect it with an access key defined in the configuration\n\nThis plugin can accept the following configuration\n\n```json\n{\n  \"PrometheusEndpoint\": {\n    \"accessKeyValue\": \"secret\", // if not defined, public access. Can be ${config.app.health.accessKey}\n    \"accessKeyQuery\": \"access_key\",\n    \"includeMetrics\": false\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"PrometheusEndpoint\" : {\n    \"accessKeyValue\" : \"${config.app.health.accessKey}\",\n    \"accessKeyQuery\" : \"access_key\",\n    \"includeMetrics\" : false\n  }\n}\n```\n\n\n\n\n\n\n## Kubernetes Ingress Controller\n\n### Infos\n\n* plugin type: `job`\n* configuration root: `KubernetesConfig`\n\n### Description\n\nThis plugin enables Otoroshi as an Ingress Controller\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Kubernetes Otoroshi CRDs Controller\n\n### Infos\n\n* plugin type: `job`\n* configuration root: `KubernetesConfig`\n\n### Description\n\nThis plugin enables Otoroshi CRDs Controller\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Kubernetes to Otoroshi certs. synchronizer\n\n### Infos\n\n* plugin type: `job`\n* configuration root: `KubernetesConfig`\n\n### Description\n\nThis plugin syncs. TLS secrets from Kubernetes to Otoroshi\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Otoroshi certs. to Kubernetes secrets synchronizer\n\n### Infos\n\n* plugin type: `job`\n* configuration root: `KubernetesConfig`\n\n### Description\n\nThis plugin syncs. Otoroshi certs to Kubernetes TLS secrets\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n### Default configuration\n\n```json\n{\n  \"KubernetesConfig\" : {\n    \"endpoint\" : \"https://kube.cluster.dev\",\n    \"token\" : \"xxx\",\n    \"userPassword\" : \"user:password\",\n    \"caCert\" : \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\n    \"trust\" : false,\n    \"namespaces\" : [ \"*\" ],\n    \"labels\" : { },\n    \"namespacesLabels\" : { },\n    \"ingressClasses\" : [ \"otoroshi\" ],\n    \"defaultGroup\" : \"default\",\n    \"ingresses\" : true,\n    \"crds\" : true,\n    \"coreDnsIntegration\" : false,\n    \"coreDnsIntegrationDryRun\" : false,\n    \"kubeLeader\" : false,\n    \"restartDependantDeployments\" : true,\n    \"watch\" : true,\n    \"syncDaikokuApikeysOnly\" : false,\n    \"kubeSystemNamespace\" : \"kube-system\",\n    \"coreDnsConfigMapName\" : \"coredns\",\n    \"coreDnsDeploymentName\" : \"coredns\",\n    \"corednsPort\" : 53,\n    \"otoroshiServiceName\" : \"otoroshi-service\",\n    \"otoroshiNamespace\" : \"otoroshi\",\n    \"clusterDomain\" : \"cluster.local\",\n    \"syncIntervalSeconds\" : 60,\n    \"coreDnsEnv\" : null,\n    \"watchTimeoutSeconds\" : 60,\n    \"watchGracePeriodSeconds\" : 5,\n    \"mutatingWebhookName\" : \"otoroshi-admission-webhook-injector\",\n    \"validatingWebhookName\" : \"otoroshi-admission-webhook-validation\",\n    \"meshDomain\" : \"otoroshi.mesh\",\n    \"openshiftDnsOperatorIntegration\" : false,\n    \"openshiftDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"openshiftDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"openshiftDnsOperatorCoreDnsPort\" : 5353,\n    \"kubeDnsOperatorIntegration\" : false,\n    \"kubeDnsOperatorCoreDnsNamespace\" : \"otoroshi\",\n    \"kubeDnsOperatorCoreDnsName\" : \"otoroshi-dns\",\n    \"kubeDnsOperatorCoreDnsPort\" : 5353,\n    \"templates\" : {\n      \"service-group\" : { },\n      \"service-descriptor\" : { },\n      \"apikeys\" : { },\n      \"global-config\" : { },\n      \"jwt-verifier\" : { },\n      \"tcp-service\" : { },\n      \"certificate\" : { },\n      \"auth-module\" : { },\n      \"script\" : { },\n      \"data-exporters\" : { },\n      \"organizations\" : { },\n      \"teams\" : { },\n      \"admins\" : { },\n      \"webhooks\" : { }\n    }\n  }\n}\n```\n\n\n\n\n\n\n## Workflow job\n\n### Infos\n\n* plugin type: `job`\n* configuration root: `WorkflowJob`\n\n### Description\n\nPeriodically run a custom workflow\n\n\n\n### Default configuration\n\n```json\n{\n  \"WorkflowJob\" : {\n    \"input\" : {\n      \"namespace\" : \"otoroshi\",\n      \"service\" : \"otoroshi-dns\"\n    },\n    \"intervalMillis\" : \"60000\",\n    \"workflow\" : {\n      \"name\" : \"some-workflow\",\n      \"description\" : \"a nice workflow\",\n      \"tasks\" : [ {\n        \"name\" : \"call-dns\",\n        \"type\" : \"http\",\n        \"request\" : {\n          \"method\" : \"PATCH\",\n          \"url\" : \"http://${env.KUBERNETES_SERVICE_HOST}:${env.KUBERNETES_SERVICE_PORT}/apis/v1/namespaces/${input.namespace}/services/${input.service}\",\n          \"headers\" : {\n            \"accept\" : \"application/json\",\n            \"content-type\" : \"application/json\",\n            \"authorization\" : \"Bearer ${file:///var/run/secrets/kubernetes.io/serviceaccount/token}\"\n          },\n          \"tls\" : {\n            \"mtls\" : true,\n            \"trustAll\" : true\n          },\n          \"body\" : [ {\n            \"op\" : \"replace\",\n            \"path\" : \"/spec/selector\",\n            \"value\" : {\n              \"app\" : \"otoroshi\",\n              \"component\" : \"dns\"\n            }\n          } ]\n        },\n        \"success\" : {\n          \"statuses\" : [ 200 ]\n        }\n      } ]\n    }\n  }\n}\n```\n\n\n\n\n\n\n\n\n"
  },
  {
    "name": "create-plugins.md",
    "id": "/plugins/create-plugins.md",
    "url": "/plugins/create-plugins.html",
    "title": "Create plugins",
    "content": "# Create plugins\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "index.md",
    "id": "/plugins/index.md",
    "url": "/plugins/index.html",
    "title": "Otoroshi plugins",
    "content": "# Otoroshi plugins\n\nIn this sections, you will find informations about Otoroshi plugins system\n\n* @ref:[Plugins system](./plugins.md)\n* @ref:[Create plugins](./create-plugins.md)\n* @ref:[Built in plugins](./built-in-plugins.md)\n\n@@@ index\n\n* [Plugins system](./plugins.md)\n* [Create plugins](./create-plugins.md)\n* [Built in plugins](./built-in-plugins.md)\n\n@@@\n"
  },
  {
    "name": "plugins.md",
    "id": "/plugins/plugins.md",
    "url": "/plugins/plugins.html",
    "title": "Otoroshi plugins system",
    "content": "# Otoroshi plugins system\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "chaos-engineering.md",
    "id": "/topics/chaos-engineering.md",
    "url": "/topics/chaos-engineering.html",
    "title": "Chaos engineering",
    "content": "# Chaos engineering\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "dev-portal.md",
    "id": "/topics/dev-portal.md",
    "url": "/topics/dev-portal.html",
    "title": "Developer portal with Daikoku",
    "content": "# Developer portal with Daikoku\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "events-and-analytics.md",
    "id": "/topics/events-and-analytics.md",
    "url": "/topics/events-and-analytics.html",
    "title": "Events and analytics",
    "content": "# Events and analytics\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "index.md",
    "id": "/topics/index.md",
    "url": "/topics/index.html",
    "title": "Detailed topics",
    "content": "# Detailed topics\n\nIn this sections, you will find informations about various Otoroshi topics  \n\n* @ref:[Chaos engineering](./chaos-engineering.md)\n* @ref:[Otoroshi's PKI](./pki.md)\n* @ref:[Input TLS](./input-tls.md)\n* @ref:[Output TLS](./output-tls.md)\n* @ref:[Monitoring](./monitoring.md)\n* @ref:[Events and analytics](./events-and-analytics.md)\n* @ref:[Developer portal with Daikoku](./dev-portal.md)\n* @ref:[Sessions management](./sessions-mgmt.md)\n* @ref:[The Otoroshi communication protocol](./otoroshi-protocol.md)\n\n@@@ index\n\n* [Chaos engineering](./chaos-engineering.md)\n* [Otoroshi's PKI](./pki.md)\n* [Input TLS](./input-tls.md)\n* [Output TLS](./output-tls.md)\n* [Monitoring](./monitoring.md)\n* [Events and analytics](./events-and-analytics.md)\n* [Developer portal with Daikoku](./dev-portal.md)\n* [Sessions management](./sessions-mgmt.md)\n* [The Otoroshi communication protocol](./otoroshi-protocol.md)\n\n@@@\n"
  },
  {
    "name": "input-tls.md",
    "id": "/topics/input-tls.md",
    "url": "/topics/input-tls.html",
    "title": "Input TLS",
    "content": "# Input TLS\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "monitoring.md",
    "id": "/topics/monitoring.md",
    "url": "/topics/monitoring.html",
    "title": "Monitoring",
    "content": "# Monitoring\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "otoroshi-protocol.md",
    "id": "/topics/otoroshi-protocol.md",
    "url": "/topics/otoroshi-protocol.html",
    "title": "The Otoroshi communication protocol",
    "content": "# The Otoroshi communication protocol\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "output-tls.md",
    "id": "/topics/output-tls.md",
    "url": "/topics/output-tls.html",
    "title": "Ouput TLS",
    "content": "# Ouput TLS\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "pki.md",
    "id": "/topics/pki.md",
    "url": "/topics/pki.html",
    "title": "Otoroshi's PKI",
    "content": "# Otoroshi's PKI\n\n@@@ warning\nTODO\n@@@"
  },
  {
    "name": "sessions-mgmt.md",
    "id": "/topics/sessions-mgmt.md",
    "url": "/topics/sessions-mgmt.html",
    "title": "Sessions management",
    "content": "# Sessions management"
  }
]